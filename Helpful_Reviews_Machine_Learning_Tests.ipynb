{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpful Reviews Machine Learning Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook presents an outline of functions and tests that were utilized to select a machine learning model to predict helpful reviews. The number of rows has been limited in order to expedite speed. Tests run with millions of rows gave similar results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROWS = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "# Import essentials\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.stats import randint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helpful_reviews_pipeline(nrows=500000, subset=True):\n",
    "    \n",
    "    # Open Dataframe\n",
    "    if subset:\n",
    "        df = pd.read_csv('df_10.csv', nrows=nrows)\n",
    "    else:\n",
    "        df = pd.read_csv('df_10.csv')\n",
    "        \n",
    "    # Rename column error\n",
    "    df.rename(columns={'Helpful?': 'Helpful'}, inplace=True)\n",
    "    \n",
    "    # Cut Middle Rows\n",
    "    df = cut_middle_rows(df)\n",
    "    \n",
    "    # Choose relevant columns\n",
    "    df = df[['reviewText', 'Helpful']]\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to eliminate middle rows\n",
    "def cut_middle_rows(df, high=0.90, low=0.5, hand_pick=True):\n",
    "\n",
    "    if not hand_pick:\n",
    "        high = df['Helpful_Rating'].median() + MIDDLEPERCENTAGE*0.25\n",
    "        low = df['Helpful_Rating'].median() - MIDDLEPERCENTAGE*0.75\n",
    "\n",
    "    df = df[(df['Helpful_Rating']<low) | (df['Helpful_Rating']>high)]\n",
    "    \n",
    "    print('Length of new dataframe: ', len(df), 'rows.')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_corpus(df):\n",
    "    \n",
    "    def normalize_document(doc):\n",
    "        # lower case and remove special characters\\whitespaces\n",
    "        doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "        doc = doc.lower()\n",
    "        doc = doc.strip()\n",
    "        # tokenize document\n",
    "        tokens = wpt.tokenize(doc)\n",
    "        # filter stopwords out of document\n",
    "        filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "        # re-create document from filtered tokens\n",
    "        doc = ' '.join(filtered_tokens)\n",
    "        return doc\n",
    "    \n",
    "    nltk.download('stopwords')\n",
    "\n",
    "    wpt = nltk.WordPunctTokenizer()\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "    corpus = df['reviewText']\n",
    "    normalize_corpus = np.vectorize(normalize_document)\n",
    "    norm_corpus = normalize_corpus(corpus)\n",
    "    \n",
    "    return norm_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X,y Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_xy(df, vectorizer):\n",
    "    vectorizer = vectorizer\n",
    "    X = vectorizer.fit_transform(df.reviewText)\n",
    "    X = X.tocsc()  # some versions of sklearn return COO format\n",
    "    y = df['Helpful']\n",
    "    return X, y\n",
    "\n",
    "def make_xy_norm(df, vectorizer, norm_corpus):\n",
    "    vectorizer = vectorizer\n",
    "    X = vectorizer.fit_transform(norm_corpus)\n",
    "    X = X.tocsc()  # some versions of sklearn return COO format\n",
    "    y = df['Helpful']\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tests(df, ml_test, vectorizer):\n",
    "    X,y = make_xy(df, vectorizer)\n",
    "    ml_test(X,y)\n",
    "    \n",
    "def run_norm_tests(df, ml_test, vectorizer, norm_corpus):\n",
    "    X,y = make_xy_norm(df, vectorizer, norm_corpus)\n",
    "    ml_test(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to return the results of machine learning tests\n",
    "def ml_classification_initial_tests(X, y, test_pct=0.15):\n",
    "        \n",
    "    #------------------------------------------------------------------------\n",
    "    \n",
    "    # NAIVE BAYES\n",
    "    print('\\nNAIVE BAYES')\n",
    "    \n",
    "    # Create a multinomial classifier\n",
    "    mnb = MultinomialNB()\n",
    "        \n",
    "    # Compute 5-fold cross-validation scores: cv_scores\n",
    "    cv_scores = cross_val_score(mnb, X, y, cv=5)\n",
    "\n",
    "    # Print the 5-fold cross-validation scores\n",
    "    print(cv_scores)\n",
    "\n",
    "    print(\"Average 5-Fold CV Score: {}\".format(np.mean(cv_scores)))\n",
    "    \n",
    "    #------------------------------------------------------------------------\n",
    "\n",
    "    \n",
    "    # LOGISTIC REGRESSION\n",
    "    print('\\nLOGISTIC REGRESSION')\n",
    "\n",
    "    # Instantiate a logistic regression classifier: logreg\n",
    "    lr = LogisticRegression()\n",
    "\n",
    "    # Compute 5-fold cross-validation scores: cv_scores\n",
    "    cv_scores = cross_val_score(lr, X, y, cv=5)\n",
    "\n",
    "    # Print the 5-fold cross-validation scores\n",
    "    print(cv_scores)\n",
    "\n",
    "    print(\"Average 5-Fold CV Score: {}\".format(np.mean(cv_scores)))\n",
    "    \n",
    "    \n",
    "    #------------------------------------------------------------------------\n",
    "    \n",
    "    # DECISION TREE TUNED\n",
    "    print('\\nDECISION TREE')\n",
    "    \n",
    "    # Instantiate a Decision Tree classifier\n",
    "    dt = DecisionTreeClassifier()\n",
    "        \n",
    "    # Compute 5-fold cross-validation scores: cv_scores\n",
    "    cv_scores = cross_val_score(dt, X, y, cv=5)\n",
    "\n",
    "    # Print the 5-fold cross-validation scores\n",
    "    print(cv_scores)\n",
    "\n",
    "    print(\"Average 5-Fold CV Score: {}\".format(np.mean(cv_scores)))\n",
    "    \n",
    "    #------------------------------------------------------------------------\n",
    "    \n",
    "    # RANDOM FORESTS\n",
    "    print('\\nRANDOM FORESTS')\n",
    "    \n",
    "    # Instantiate a Random Forest Classifier\n",
    "    rfc = RandomForestClassifier()\n",
    "    \n",
    "    # Compute 5-fold cross-validation scores: cv_scores\n",
    "    cv_scores = cross_val_score(rfc, X, y, cv=5)\n",
    "\n",
    "    # Print the 5-fold cross-validation scores\n",
    "    print(cv_scores)\n",
    "\n",
    "    print(\"Average 5-Fold CV Score: {}\".format(np.mean(cv_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(ROWS, pipeline):\n",
    "    \n",
    "    df = pipeline(nrows=ROWS)\n",
    "    \n",
    "    split = int(len(df)*0.9)\n",
    "    \n",
    "    df_train = df[:split]\n",
    "    print('Length of df_train:', len(df_train))\n",
    "    \n",
    "    df_test = df[split:]\n",
    "    print('Length of df_test:', len(df_test))\n",
    "    \n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of new dataframe:  42196 rows.\n",
      "Length of df_train: 37976\n",
      "Length of df_test: 4220\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = split_data(ROWS, helpful_reviews_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/coreyjwade/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "\n",
      "NAIVE BAYES\n",
      "[0.82622433 0.85215903 0.85174457 0.84094799 0.84685278]\n",
      "Average 5-Fold CV Score: 0.8435857398135311\n",
      "\n",
      "LOGISTIC REGRESSION\n"
     ]
    }
   ],
   "source": [
    "norm_corpus = make_corpus(df_train)\n",
    "run_tests(df_train, ml_classification_initial_tests, CountVectorizer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes, Logistic Regression and Random Forests are all worth pursuing going forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperParameter Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to return the results of machine learning tests\n",
    "def ml_classification_tests(X, y, test_pct=0.15):\n",
    "        \n",
    "    # Split into training and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_pct)\n",
    "    \n",
    "    #------------------------------------------------------------------------\n",
    "    \n",
    "    # NAIVE BAYES\n",
    "    print('\\nNAIVE BAYES')\n",
    "    \n",
    "    #the grid of parameters to search over\n",
    "    alphas = [0.001, 0.01, .1, 1, 5]\n",
    "    \n",
    "    param_grid = {'alpha': alphas}\n",
    "    \n",
    "    # Create a multinomial classifier\n",
    "    mnb = MultinomialNB()\n",
    "    \n",
    "    mnb_cv = GridSearchCV(mnb, param_grid, cv=5)\n",
    "    \n",
    "    # Fit the classifier to the data\n",
    "    mnb_cv.fit(X_train, y_train)\n",
    "    \n",
    "    # Print the tuned parameters and score\n",
    "    print(\"Best Naive Bayes alpha: {}\".format(mnb_cv.best_params_)) \n",
    "    print(\"Best Naive Bayes score: {}\".format(mnb_cv.best_score_))\n",
    "    \n",
    "    # Predict the labels of the test set: y_pred\n",
    "    y_pred = mnb_cv.predict(X_test)\n",
    "\n",
    "    # Compute and print the confusion matrix and classification report\n",
    "    print('Confusion Matrix:', confusion_matrix(y_test, y_pred))\n",
    "    print('Classification Report:', classification_report(y_test, y_pred))\n",
    "    \n",
    "    #------------------------------------------------------------------------\n",
    "    \n",
    "    # LOGISTIC REGRESSION\n",
    "    print('\\nLOGISTIC REGRESSION')\n",
    "    \n",
    "    # Setup the hyperparameter grid\n",
    "    c_space = np.logspace(-5, 8, 10)\n",
    "    param_grid = {'C': c_space}\n",
    "\n",
    "    # Instantiate a logistic regression classifier: logreg\n",
    "    logreg = LogisticRegression()\n",
    "\n",
    "    # Instantiate the GridSearchCV object: logreg_cv\n",
    "    logreg_cv = GridSearchCV(logreg, param_grid, cv=5)\n",
    "\n",
    "    # Fit it to the data\n",
    "    logreg_cv.fit(X_train,y_train)\n",
    "\n",
    "    # Print the tuned parameters and score\n",
    "    print(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_)) \n",
    "    print(\"Best Logistic Regression score: {}\".format(logreg_cv.best_score_))\n",
    "    \n",
    "        # Predict the labels of the test set: y_pred\n",
    "    y_pred = logreg_cv.predict(X_test)\n",
    "\n",
    "    # Compute and print the confusion matrix and classification report\n",
    "    print('Confusion Matrix:', confusion_matrix(y_test, y_pred))\n",
    "    print('Classification Report:', classification_report(y_test, y_pred))    \n",
    "   \n",
    "    #------------------------------------------------------------------------\n",
    "    \n",
    "    # RANDOM FORESTS\n",
    "    print('\\nRANDOM FORESTS')\n",
    "    \n",
    "    # Number of trees in random forest\n",
    "    n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 10)]\n",
    "    # Number of features to consider at every split\n",
    "    max_features = ['auto', 'sqrt']\n",
    "    # Maximum number of levels in tree\n",
    "    max_depth = [int(x) for x in np.linspace(10, 100, num = 10)]\n",
    "    max_depth.append(None)\n",
    "    # Minimum number of samples required to split a node\n",
    "    min_samples_split = [2, 5, 10]\n",
    "    # Minimum number of samples required at each leaf node\n",
    "    min_samples_leaf = [1, 2, 4]\n",
    "    # Method of selecting samples for training each tree\n",
    "    bootstrap = [True, False]\n",
    "    # Create the random grid\n",
    "    random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}    \n",
    "    \n",
    "    # Instantiate a Random Forest Classifier\n",
    "    rfc = RandomForestClassifier()\n",
    "    \n",
    "    # Instantiate RandomizedSearchCV\n",
    "    rf_random = RandomizedSearchCV(estimator = rfc, param_distributions = random_grid, cv = 5, n_jobs = -1, n_iter=5)\n",
    "    \n",
    "    # Fit the random search model\n",
    "    rf_random.fit(X_train, y_train)\n",
    "    \n",
    "    # Print the tuned parameters and score\n",
    "    print(\"Tuned Random Forest Parameters: {}\".format(rf_random.best_params_))\n",
    "    print(\"Best Random Forest score: {}\".format(rf_random.best_score_))\n",
    "    \n",
    "    # Predict the labels of the test set: y_pred\n",
    "    y_pred = mnb_cv.predict(X_test)\n",
    "\n",
    "    # Compute and print the confusion matrix and classification report\n",
    "    print('Confusion Matrix:', confusion_matrix(y_test, y_pred))\n",
    "    print('Classification Report:', classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_tests(df_train, ml_classification_tests, CountVectorizer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression is clearly outperforming Random Forests and Naive Bayes with C=0.007742636826811269.\n",
    "\n",
    "The logistic regression function below can be used for subsequent tests. The Confusion Matrix is essential due to the class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X, y, test_pct=0.15):\n",
    "    \n",
    "    # Split into training and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_pct)\n",
    "    \n",
    "    # LOGISTIC REGRESSION\n",
    "    print('\\nLOGISTIC REGRESSION')\n",
    "\n",
    "    # Instantiate a logistic regression classifier: logreg\n",
    "    logreg = LogisticRegression(C=0.007742636826811269)\n",
    "\n",
    "    # Fit it to the data\n",
    "    logreg.fit(X_train,y_train)\n",
    "    \n",
    "    # Predict the labels of the test set: y_pred\n",
    "    y_pred = logreg.predict(X_test)\n",
    "\n",
    "    # Compute and print the confusion matrix and classification report\n",
    "    print('Confusion Matrix:', confusion_matrix(y_test, y_pred))\n",
    "    print('Classification Report:', classification_report(y_test, y_pred))\n",
    "    \n",
    "    return logreg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each standard vectorizer, CountVectorizer, and TfidfVectorizer, there are various n_gram options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_vectorizer(df, ml_test, norm_corpus):\n",
    "    vectorizers = [CountVectorizer(), CountVectorizer(ngram_range=(1,2)), CountVectorizer(ngram_range=(1,3)), TfidfVectorizer(min_df=0.), TfidfVectorizer(ngram_range=(1, 2), min_df=0.), TfidfVectorizer(ngram_range=(1, 3), min_df=0.) ]\n",
    "    for vect in vectorizers:\n",
    "        print(str(vect))\n",
    "        print('\\nmake_xy')\n",
    "        run_tests(df, ml_test, vect)\n",
    "        print('\\nmake_xy_norm')\n",
    "        run_norm_tests(df, ml_test, vect, norm_corpus)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choose_vectorizer(df_train, logistic_regression, norm_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make_xy is consistently outperforming make_xy-norm. CountVectorizer(n_gram=(1,3)) is currently best, but CountVectorizer(n_gram=(1,2)) is close and faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizer Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression has a min_df parameter that can be tuned to obtain better results. When between 0 and 1, it discounts the percentage of frequency of words. For instance, df_min = 0.01 would discount words that appear in less than 1% of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from six.moves import range\n",
    "\n",
    "# Setup Seaborn\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Cumulative Frequency function\n",
    "def ecdf(data):\n",
    "    \"\"\"Compute ECDF for a one-dimensional array of measurements.\"\"\"\n",
    "\n",
    "    # Number of data points: n\n",
    "    n = len(data)\n",
    "\n",
    "    # x-data for the ECDF: x\n",
    "    x = np.sort(data)\n",
    "\n",
    "    # y-data for the ECDF: y\n",
    "    y = np.arange(1, n+1) / n\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn X (df.reviewText) into one dimensional array\n",
    "def sort_reviews_into_1D_list(df, vectorizer):\n",
    "    X = vectorizer.fit_transform(df.reviewText)\n",
    "    review_list = list(sorted((X > 0).sum(axis=0).reshape(-1).tolist()[0]))\n",
    "    return review_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_list = sort_reviews_into_1D_list(df_train, CountVectorizer(ngram_range=(1,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ECDF\n",
    "x1, y1 = ecdf(review_list)\n",
    "\n",
    "# Generate plot\n",
    "plt.plot(x1, y1, marker='.', linestyle='none')\n",
    "\n",
    "# Make the margins nice\n",
    "plt.margins(.02)\n",
    "\n",
    "# Label the axes\n",
    "plt.xlabel('Word Count')\n",
    "plt.ylabel('ECDF')\n",
    "plt.title('Word Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate reduced plot\n",
    "plt.plot(x1, y1, marker='.', linestyle='none')\n",
    "\n",
    "# Make the margins nice\n",
    "plt.margins(.02)\n",
    "\n",
    "# Label the axes\n",
    "plt.xlabel('Word Count')\n",
    "plt.ylabel('ECDF')\n",
    "\n",
    "#Limit axes\n",
    "plt.xlim(0,80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate reduced plot\n",
    "plt.plot(x1, y1, marker='.', linestyle='none')\n",
    "\n",
    "# Make the margins nice\n",
    "plt.margins(.02)\n",
    "\n",
    "# Label the axes\n",
    "plt.xlabel('Word Count')\n",
    "plt.ylabel('ECDF')\n",
    "\n",
    "#Limit axes\n",
    "plt.xlim(0,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph above indicates that 75% of the distinct one and two-word combinations in the entire corpus only appear once. The graph suggests that we try df_mins of 2-10.\n",
    "\n",
    "A df_min of 2 means that the classifer will discount all words that appear in less than 2 reviews. There is also an option to use percentages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjust min_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_df(df, test):\n",
    "    min_dfs = [1, 2, 4, 6, 8, 10]\n",
    "    for val in min_dfs:\n",
    "        vect = CountVectorizer(ngram_range=(1,2), min_df=val)\n",
    "        print(vect)\n",
    "        run_tests(df, test, vect)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_df(df_train, logistic_regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best result on a 250,000 subset comes from min_df = 2, which is very close to min_df=1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use the new dataset, df_overall_2, to determine a max_df since it's larger than the previous dataset. A max_df of 0.99 would discount words that appear in more than 99% of all documents, presumably words like 'a', 'the', and other common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate max plot\n",
    "plt.plot(x1, y1, marker='.', linestyle='none')\n",
    "\n",
    "# Make the margins nice\n",
    "plt.margins(.02)\n",
    "\n",
    "# Label the axes\n",
    "plt.xlabel('Word Count')\n",
    "plt.ylabel('ECDF')\n",
    "plt.title('Word Frequency')\n",
    "\n",
    "#Limit axes\n",
    "plt.ylim(0.95, 1.001)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate zoom plot\n",
    "plt.plot(x1, y1, marker='.', linestyle='none')\n",
    "\n",
    "# Make the margins nice\n",
    "plt.margins(.02)\n",
    "\n",
    "# Label the axes\n",
    "plt.xlabel('Word Count')\n",
    "plt.ylabel('ECDF')\n",
    "plt.title('Word Frequency')\n",
    "\n",
    "#Limit axes\n",
    "plt.ylim(0.995, 1.001)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph really changes at around 0.999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_df(df, test):\n",
    "    max_dfs = [0.9, 0.99, 0.999, 0.9999, 1.0]\n",
    "    for val in max_dfs:\n",
    "        vect = CountVectorizer(ngram_range=(1,2), max_df=val)\n",
    "        print(vect)\n",
    "        run_tests(df, test, vect)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_df(df_train, logistic_regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best results for df_train are max_df=1.0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Combined Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "def log_reg_fin(X, y):\n",
    "        \n",
    "    lr = LogisticRegression(C=0.007742636826811269)\n",
    "    \n",
    "    lr.fit(X, y)\n",
    "    \n",
    "    scores = cross_val_score(lr, X, y, cv=5)\n",
    "\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "        \n",
    "    predicted = cross_val_predict(lr, X, y, cv=5)\n",
    "    \n",
    "    report = classification_report(y, predicted) \n",
    "    \n",
    "    print('\\n')    \n",
    "    print(report)\n",
    "    \n",
    "    joblib.dump(lr, 'lr_model.pkl')\n",
    "    print('Logistic Regression model saved as \"lr_model.pkl\"')\n",
    "    \n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_tests(df_train, log_reg_fin, CountVectorizer(ngram_range=(1,3), max_df=1.0, min_df=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_tests(df_test, log_reg_fin, CountVectorizer(ngram_range=(1,3), max_df=1.0, min_df=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Star Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's worth running the same tests to try and determine if someone likes or dislikes a book basked on the review. This can translate directly to the number of stars given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Stars Column (binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_10 = pd.read_csv('df_10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function\n",
    "def stars(row):\n",
    "    # Give 1 star reviews a value of 0\n",
    "    if row['overall']==1:\n",
    "        return 0\n",
    "    # Give 2 star reviews a value of 0\n",
    "    elif row['overall']==2:\n",
    "        return 0\n",
    "    # Give 3 star reviews a value of 0\n",
    "    elif row['overall']==3:\n",
    "        return 0\n",
    "    # Give 4,5 star reviews a value of 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# Create column\n",
    "df_10['Stars'] = df_10.apply(stars, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save updated file\n",
    "df_10.to_csv('df_10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define new dataframe that eliminates the middle, 3 and 4-star reviews\n",
    "#df_stars = df_10[(df_10['overall']!=3) | (df_10['overall']!=4)]\n",
    "df_stars = df_10[(df_10['overall']!=3)]\n",
    "\n",
    "# Save updated file\n",
    "df_stars.to_csv('df_stars.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def star_reviews_pipeline(nrows=10000, subset=True):\n",
    "    \n",
    "    # Open Dataframe\n",
    "    if subset:\n",
    "        df = pd.read_csv('df_stars.csv', nrows=nrows)\n",
    "    else:\n",
    "        df = pd.read_csv('df_stars.csv')\n",
    "    \n",
    "    # Choose relevant columns\n",
    "    df = df[['reviewText', 'Stars']]\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjust Functions for Initial Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_xy_stars(df, vectorizer):\n",
    "    vectorizer = vectorizer\n",
    "    X = vectorizer.fit_transform(df.reviewText)\n",
    "    X = X.tocsc()  # some versions of sklearn return COO format\n",
    "    y = df.Stars\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_star_tests(df, ml_test, vectorizer):\n",
    "    X,y = make_xy_stars(df, vectorizer)\n",
    "    ml_test(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_stars, df_test_stars = split_data(ROWS, star_reviews_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_star_tests(df_train_stars, logistic_regression, CountVectorizer(ngram_range=(1,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_star_tests(df_test_stars, logistic_regression, CountVectorizer(ngram_range=(1,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is definitely worth pursuing going forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best result:\n",
    "LogisticRegression(C=0.007742636826811269)\n",
    "CountVectorizer(ngram_range=(1,2))\n",
    "Precision, Recall and F1 Scores: 91%, 91%, 91%\n",
    "\n",
    "The results are very compelling. With some hyperparameter tweaking, logistic regression reaches 90% accuracy with CountVectorizer. Multiple tests revealed the same results. Over multiple test-sizes, CountVectorizer(ngram_range=1,2) and CountVectorizer(ngram_range=1,3) with min_df = 2, or 1 and max_df = 0.9999 or 1 were best.\n",
    "\n",
    "Tfidf has gone down in performance from a couple months ago, so further investigation is required. When performing optimally, however, it still did not outperform CountVectorizer. \n",
    "\n",
    "Another consistent test result is that make_xy outperform makes_xy_norm. This means that the general corpus generated by CountVectorizer does better than the normed corpus that I created."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
