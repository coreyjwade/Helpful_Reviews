{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpful Reviews Machine Learning Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOTEFILTER = 10\n",
    "MIDDLEPERCENTAGE = 0.5\n",
    "ROWS = 200000\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Open DataFrames\n",
    "df = pd.read_csv('df.csv', nrows=ROWS)\n",
    "\n",
    "# Minimize dataset\n",
    "df = df[df['Total_Votes']>=VOTEFILTER]\n",
    "\n",
    "# Choose relevant columns\n",
    "df = df[['reviewText', 'overall', 'helpful', 'Review_Length', 'Sentence_Length', 'Word_Length', 'Helpful_Rating', 'Helpful' ]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lose middle rows\n",
    "hand_pick_percentage = False\n",
    "\n",
    "if hand_pick_percentage:\n",
    "    high = 0.85\n",
    "    low = 0.50\n",
    "else:\n",
    "    high = df['Helpful_Rating'].median() + MIDDLEPERCENTAGE*0.25\n",
    "    low = df['Helpful_Rating'].median() - MIDDLEPERCENTAGE*0.75\n",
    "\n",
    "df_adjusted = df[(df['Helpful_Rating']<low) | (df['Helpful_Rating']>high)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>Review_Length</th>\n",
       "      <th>Sentence_Length</th>\n",
       "      <th>Word_Length</th>\n",
       "      <th>Helpful_Rating</th>\n",
       "      <th>Helpful</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>52769.000000</td>\n",
       "      <td>52769.000000</td>\n",
       "      <td>52769.000000</td>\n",
       "      <td>52769.000000</td>\n",
       "      <td>52769.000000</td>\n",
       "      <td>52769.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.442931</td>\n",
       "      <td>1683.301067</td>\n",
       "      <td>18.090618</td>\n",
       "      <td>4.588956</td>\n",
       "      <td>0.682575</td>\n",
       "      <td>0.487369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.553135</td>\n",
       "      <td>1615.423118</td>\n",
       "      <td>6.119123</td>\n",
       "      <td>0.368717</td>\n",
       "      <td>0.225270</td>\n",
       "      <td>0.499845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>1.518519</td>\n",
       "      <td>2.964286</td>\n",
       "      <td>0.021653</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>649.000000</td>\n",
       "      <td>14.100000</td>\n",
       "      <td>4.349112</td>\n",
       "      <td>0.552283</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>1240.000000</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>4.582222</td>\n",
       "      <td>0.737667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>2203.000000</td>\n",
       "      <td>21.285714</td>\n",
       "      <td>4.822917</td>\n",
       "      <td>0.867983</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>31858.000000</td>\n",
       "      <td>163.500000</td>\n",
       "      <td>8.090909</td>\n",
       "      <td>0.992464</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            overall  Review_Length  Sentence_Length   Word_Length  \\\n",
       "count  52769.000000   52769.000000     52769.000000  52769.000000   \n",
       "mean       3.442931    1683.301067        18.090618      4.588956   \n",
       "std        1.553135    1615.423118         6.119123      0.368717   \n",
       "min        1.000000      20.000000         1.518519      2.964286   \n",
       "25%        2.000000     649.000000        14.100000      4.349112   \n",
       "50%        4.000000    1240.000000        17.400000      4.582222   \n",
       "75%        5.000000    2203.000000        21.285714      4.822917   \n",
       "max        5.000000   31858.000000       163.500000      8.090909   \n",
       "\n",
       "       Helpful_Rating       Helpful  \n",
       "count    52769.000000  52769.000000  \n",
       "mean         0.682575      0.487369  \n",
       "std          0.225270      0.499845  \n",
       "min          0.021653      0.000000  \n",
       "25%          0.552283      0.000000  \n",
       "50%          0.737667      0.000000  \n",
       "75%          0.867983      1.000000  \n",
       "max          0.992464      1.000000  "
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reviewText         19686\n",
       "overall            19686\n",
       "helpful            19686\n",
       "Review_Length      19686\n",
       "Sentence_Length    19686\n",
       "Word_Length        19686\n",
       "Helpful_Rating     19686\n",
       "Helpful            19686\n",
       "dtype: int64"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_adjusted.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.stats import randint\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tester(X, y):\n",
    "    \n",
    "    # Split into training and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    \n",
    "    #------------------------------------------------------------------------\n",
    "    \n",
    "    # NAIVE BAYES\n",
    "    \n",
    "    # Create a multinomial classifier\n",
    "    clf = MultinomialNB()\n",
    "    \n",
    "    # Fit the classifier to the data\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Print the accuracy\n",
    "    print(\"Naive Bayes Multinomial Training CLF score:\", clf.score(X_train, y_train))\n",
    "    print(\"Naive Bayes Multinomial Test CLF score:\", clf.score(X_test, y_test))\n",
    "    \n",
    "    # Compute 5-fold cross-validation scores: cv_scores\n",
    "    cv_scores = cross_val_score(clf, X, y, cv=5)\n",
    "    \n",
    "    # Print cross-validation scores\n",
    "    print('Naive Bayes Multinomial Cross-validation scores:', cv_scores)\n",
    "    print('Naive Bayes Multinomial Mean cross-validation scores:', np.mean(cv_scores))    \n",
    "    \n",
    "    #------------------------------------------------------------------------\n",
    "\n",
    "    \n",
    "    # LOGISTIC REGRESSION\n",
    "    \n",
    "    # Create the classifier: logreg\n",
    "    logreg = LogisticRegression()\n",
    "\n",
    "    # Fit the classifier to the training data\n",
    "    logreg.fit(X_train, y_train)\n",
    "    \n",
    "    # Compute predicted probabilities: y_pred_prob\n",
    "    y_pred_prob = logreg.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    # Compute and print the confusion matrix and classification report\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Compute cross-validated AUC scores: cv_auc\n",
    "    cv_auc = cross_val_score(logreg, X, y, cv=5, scoring='roc_auc')\n",
    "    \n",
    "    # Print list of AUC scores\n",
    "    print(\"Logistic Regression AUC cross-validation: {}\".format(cv_auc))\n",
    "    \n",
    "    print(\"Logistic Regression AUC cross-validation mean: {}\".format(np.mean(cv_auc)))\n",
    "    \n",
    "    #------------------------------------------------------------------------\n",
    "\n",
    "    # Logistic Regression Tuned commented out because it was taking too long and not adding significant data. It will be reused later.\n",
    "    \n",
    "    # LOGISTIC REGRESSION TUNED\n",
    "    \n",
    "    # Setup the hyperparameter grid\n",
    "    #c_space = np.logspace(-5, 8, 15)\n",
    "    #param_grid = {'C': c_space}\n",
    "\n",
    "    # Instantiate a logistic regression classifier: logreg\n",
    "    #logreg = LogisticRegression()\n",
    "\n",
    "    # Instantiate the GridSearchCV object: logreg_cv\n",
    "    #logreg_cv = GridSearchCV(logreg, param_grid, cv=5)\n",
    "\n",
    "    # Fit it to the data\n",
    "    #logreg_cv.fit(X,y)\n",
    "\n",
    "    # Print the tuned parameters and score\n",
    "    #print(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_)) \n",
    "    #print(\"Best Tuned Logistic Regression score: {}\".format(logreg_cv.best_score_))\n",
    "    \n",
    "    #------------------------------------------------------------------------\n",
    "\n",
    "    \n",
    "    # DECISION TREE TUNED\n",
    "    \n",
    "    # Setup the parameters and distributions to sample from: param_dist\n",
    "    param_dist = {\"max_depth\": [3, None],\n",
    "              \"max_features\": randint(1, 9),\n",
    "              \"min_samples_leaf\": randint(1, 9),\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "    # Instantiate a Decision Tree classifier: tree\n",
    "    tree = DecisionTreeClassifier()\n",
    "\n",
    "    # Instantiate the RandomizedSearchCV object: tree_cv\n",
    "    tree_cv = RandomizedSearchCV(tree, param_dist, cv=5)\n",
    "\n",
    "    # Fit it to the data\n",
    "    tree_cv.fit(X,y)\n",
    "\n",
    "    # Print the tuned parameters and score\n",
    "    print(\"Tuned Decision Tree Parameters: {}\".format(tree_cv.best_params_))\n",
    "    print(\"Best Tuned Decision Tree score: {}\".format(tree_cv.best_score_))\n",
    "    \n",
    "    \n",
    "    clf = RandomForestClassifier()\n",
    "\n",
    "    # Fit the classifier to the data\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Print the accuracy\n",
    "    print(\"Random Forests Training score:\", clf.score(X_train, y_train))\n",
    "    print(\"Random Forests Test CLF score:\", clf.score(X_test, y_test))\n",
    "    \n",
    "    # Compute 5-fold cross-validation scores: cv_scores\n",
    "    cv_scores = cross_val_score(clf, X, y, cv=5)\n",
    "\n",
    "    print('Random Forest cv_scores:', cv_scores)\n",
    "    print('Random Forest cv_mean:', np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/coreyjwade/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "corpus = df_adjusted['reviewText']\n",
    "normalize_corpus = np.vectorize(normalize_document)\n",
    "norm_corpus = normalize_corpus(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X,y Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_xy(df, vectorizer):\n",
    "    vectorizer = vectorizer\n",
    "    X = vectorizer.fit_transform(df.reviewText)\n",
    "    X = X.tocsc()  # some versions of sklearn return COO format\n",
    "    y = df.Helpful\n",
    "    return X, y\n",
    "\n",
    "def make_xy_norm(df, vectorizer):\n",
    "    vectorizer = vectorizer\n",
    "    X = vectorizer.fit_transform(norm_corpus)\n",
    "    X = X.tocsc()  # some versions of sklearn return COO format\n",
    "    y = df.Helpful\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tests(vectorizer, xy_func):\n",
    "    X,y = xy_func(df_adjusted, vectorizer)\n",
    "    tester(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests Round 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VOTEFILTER = 10\n",
    "MIDDLEPERCENTAGE = 0.5\n",
    "ROWS = 200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "<function make_xy at 0x1a2af5d2f0>\n",
      "Naive Bayes Training CLF score: 0.8825812712275595\n",
      "Naive Bayes Test CLF score: 0.8100897404802329\n",
      "Logistic Regression AUC: 0.8988391801702038\n",
      "Logistic Regression AUC scores computed using 5-fold cross-validation: [0.85135184 0.8892506  0.86934367 0.89447709 0.89814798]\n",
      "Tuned Decision Tree Parameters: {'criterion': 'gini', 'max_depth': None, 'max_features': 7, 'min_samples_leaf': 6}\n",
      "Best Tuned Decision Tree score: 0.7607495906361817\n",
      "\n",
      "\n",
      "<function make_xy_norm at 0x1a2af5dea0>\n",
      "Naive Bayes Training CLF score: 0.8309073265405144\n",
      "Naive Bayes Test CLF score: 0.7933543536260005\n",
      "Logistic Regression AUC: 0.8993271207764677\n",
      "Logistic Regression AUC scores computed using 5-fold cross-validation: [0.85476487 0.8878056  0.87177538 0.89133884 0.89880898]\n",
      "Tuned Decision Tree Parameters: {'criterion': 'entropy', 'max_depth': 3, 'max_features': 3, 'min_samples_leaf': 3}\n",
      "Best Tuned Decision Tree score: 0.7606282976529808\n",
      "\n",
      "\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "<function make_xy at 0x1a2af5d2f0>\n",
      "Naive Bayes Training CLF score: 0.7988031699822092\n",
      "Naive Bayes Test CLF score: 0.7678874605869512\n",
      "Logistic Regression AUC: 0.9126044518080968\n",
      "Logistic Regression AUC scores computed using 5-fold cross-validation: [0.86857944 0.90915854 0.88896089 0.91310746 0.91255208]\n",
      "Tuned Decision Tree Parameters: {'criterion': 'entropy', 'max_depth': None, 'max_features': 8, 'min_samples_leaf': 7}\n",
      "Best Tuned Decision Tree score: 0.7605676511613804\n",
      "\n",
      "\n",
      "<function make_xy_norm at 0x1a2af5dea0>\n",
      "Naive Bayes Training CLF score: 0.886220281416788\n",
      "Naive Bayes Test CLF score: 0.7635217074945428\n",
      "Logistic Regression AUC: 0.9069234840169171\n",
      "Logistic Regression AUC scores computed using 5-fold cross-validation: [0.86159749 0.89994196 0.88446944 0.90254456 0.90874019]\n",
      "Tuned Decision Tree Parameters: {'criterion': 'entropy', 'max_depth': None, 'max_features': 2, 'min_samples_leaf': 5}\n",
      "Best Tuned Decision Tree score: 0.7605676511613804\n",
      "\n",
      "\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(2, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "<function make_xy at 0x1a2af5d2f0>\n",
      "Naive Bayes Training CLF score: 0.89115316189552\n",
      "Naive Bayes Test CLF score: 0.7724957555178268\n",
      "Logistic Regression AUC: 0.9008310119068609\n",
      "Logistic Regression AUC scores computed using 5-fold cross-validation: [0.8555509  0.9001605  0.87738528 0.89829201 0.90439213]\n",
      "Tuned Decision Tree Parameters: {'criterion': 'gini', 'max_depth': 3, 'max_features': 6, 'min_samples_leaf': 8}\n",
      "Best Tuned Decision Tree score: 0.7605676511613804\n",
      "\n",
      "\n",
      "<function make_xy_norm at 0x1a2af5dea0>\n",
      "Naive Bayes Training CLF score: 0.9904577066149118\n",
      "Naive Bayes Test CLF score: 0.3528983749696823\n",
      "Logistic Regression AUC: 0.8576536276156232\n",
      "Logistic Regression AUC scores computed using 5-fold cross-validation: [0.78471477 0.85532095 0.82788545 0.83727914 0.8737869 ]\n",
      "Tuned Decision Tree Parameters: {'criterion': 'entropy', 'max_depth': None, 'max_features': 2, 'min_samples_leaf': 7}\n",
      "Best Tuned Decision Tree score: 0.7605676511613804\n",
      "\n",
      "\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "<function make_xy at 0x1a2af5d2f0>\n",
      "Naive Bayes Training CLF score: 0.8718259744460618\n",
      "Naive Bayes Test CLF score: 0.7569730778559302\n",
      "Logistic Regression AUC: 0.9111209507306317\n",
      "Logistic Regression AUC scores computed using 5-fold cross-validation: [0.8712241  0.91151051 0.8906214  0.91412929 0.91316608]\n",
      "Tuned Decision Tree Parameters: {'criterion': 'entropy', 'max_depth': None, 'max_features': 2, 'min_samples_leaf': 8}\n",
      "Best Tuned Decision Tree score: 0.7605676511613804\n",
      "\n",
      "\n",
      "<function make_xy_norm at 0x1a2af5dea0>\n",
      "Naive Bayes Training CLF score: 0.9870612971049653\n",
      "Naive Bayes Test CLF score: 0.7635217074945428\n",
      "Logistic Regression AUC: 0.894775997435301\n",
      "Logistic Regression AUC scores computed using 5-fold cross-validation: [0.86213026 0.90140664 0.88540518 0.90330057 0.90898226]\n",
      "Tuned Decision Tree Parameters: {'criterion': 'entropy', 'max_depth': None, 'max_features': 3, 'min_samples_leaf': 3}\n",
      "Best Tuned Decision Tree score: 0.7605676511613804\n",
      "\n",
      "\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(2, 3), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "<function make_xy at 0x1a2af5d2f0>\n",
      "Naive Bayes Training CLF score: 0.9787320071162866\n",
      "Naive Bayes Test CLF score: 0.7635217074945428\n",
      "Logistic Regression AUC: 0.8966104907963832\n",
      "Logistic Regression AUC scores computed using 5-fold cross-validation: [0.85379823 0.89774544 0.8732981  0.89250874 0.90056711]\n",
      "Tuned Decision Tree Parameters: {'criterion': 'entropy', 'max_depth': None, 'max_features': 4, 'min_samples_leaf': 2}\n",
      "Best Tuned Decision Tree score: 0.7605676511613804\n",
      "\n",
      "\n",
      "<function make_xy_norm at 0x1a2af5dea0>\n",
      "Naive Bayes Training CLF score: 0.9992721979621543\n",
      "Naive Bayes Test CLF score: 0.23963133640552994\n",
      "Logistic Regression AUC: 0.8316496024746275\n",
      "Logistic Regression AUC scores computed using 5-fold cross-validation: [0.77557451 0.84712136 0.82032231 0.83052357 0.86858125]\n",
      "Tuned Decision Tree Parameters: {'criterion': 'entropy', 'max_depth': 3, 'max_features': 8, 'min_samples_leaf': 4}\n",
      "Best Tuned Decision Tree score: 0.7605676511613804\n",
      "\n",
      "\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(3, 3), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "<function make_xy at 0x1a2af5d2f0>\n",
      "Naive Bayes Training CLF score: 0.9964418567038654\n",
      "Naive Bayes Test CLF score: 0.3766674751394616\n",
      "Logistic Regression AUC: 0.8546992724238044\n",
      "Logistic Regression AUC scores computed using 5-fold cross-validation: [0.81130311 0.86021541 0.84117962 0.84993395 0.8694358 ]\n",
      "Tuned Decision Tree Parameters: {'criterion': 'entropy', 'max_depth': None, 'max_features': 8, 'min_samples_leaf': 5}\n",
      "Best Tuned Decision Tree score: 0.7605676511613804\n",
      "\n",
      "\n",
      "<function make_xy_norm at 0x1a2af5dea0>\n",
      "Naive Bayes Training CLF score: 0.9995956655345302\n",
      "Naive Bayes Test CLF score: 0.2372059180208586\n",
      "Logistic Regression AUC: 0.7461384872116202\n",
      "Logistic Regression AUC scores computed using 5-fold cross-validation: [0.65948509 0.7347001  0.71023787 0.73076043 0.7397249 ]\n",
      "Tuned Decision Tree Parameters: {'criterion': 'gini', 'max_depth': 3, 'max_features': 2, 'min_samples_leaf': 8}\n",
      "Best Tuned Decision Tree score: 0.7605676511613804\n",
      "\n",
      "\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=0.0,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "<function make_xy at 0x1a2af5d2f0>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Training CLF score: 0.7625748018761119\n",
      "Naive Bayes Test CLF score: 0.754790201309726\n",
      "Logistic Regression AUC: 0.8999393964804219\n",
      "Logistic Regression AUC scores computed using 5-fold cross-validation: [0.84814819 0.8980301  0.8818843  0.89923348 0.90059793]\n",
      "Tuned Decision Tree Parameters: {'criterion': 'gini', 'max_depth': None, 'max_features': 5, 'min_samples_leaf': 4}\n",
      "Best Tuned Decision Tree score: 0.7609315301109831\n",
      "\n",
      "\n",
      "<function make_xy_norm at 0x1a2af5dea0>\n",
      "Naive Bayes Training CLF score: 0.7595018599385411\n",
      "Naive Bayes Test CLF score: 0.7637642493330099\n",
      "Logistic Regression AUC: 0.8921045340247901\n",
      "Logistic Regression AUC scores computed using 5-fold cross-validation: [0.83679967 0.88953526 0.88159459 0.89660918 0.89468075]\n",
      "Tuned Decision Tree Parameters: {'criterion': 'entropy', 'max_depth': 3, 'max_features': 2, 'min_samples_leaf': 1}\n",
      "Best Tuned Decision Tree score: 0.7606282976529808\n",
      "\n",
      "\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=0.0,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "<function make_xy at 0x1a2af5d2f0>\n",
      "Naive Bayes Training CLF score: 0.7611191978004205\n",
      "Naive Bayes Test CLF score: 0.7591559544021343\n",
      "Logistic Regression AUC: 0.9005228291330045\n",
      "Logistic Regression AUC scores computed using 5-fold cross-validation: [0.85408782 0.90010599 0.87704914 0.89620237 0.90134687]\n",
      "Tuned Decision Tree Parameters: {'criterion': 'entropy', 'max_depth': None, 'max_features': 5, 'min_samples_leaf': 3}\n",
      "Best Tuned Decision Tree score: 0.7605676511613804\n",
      "\n",
      "\n",
      "<function make_xy_norm at 0x1a2af5dea0>\n",
      "Naive Bayes Training CLF score: 0.7620087336244541\n",
      "Naive Bayes Test CLF score: 0.7564879941789959\n",
      "Logistic Regression AUC: 0.8985692369987827\n",
      "Logistic Regression AUC scores computed using 5-fold cross-validation: [0.84434315 0.89706812 0.8806851  0.89686792 0.89995462]\n",
      "Tuned Decision Tree Parameters: {'criterion': 'gini', 'max_depth': 3, 'max_features': 7, 'min_samples_leaf': 2}\n",
      "Best Tuned Decision Tree score: 0.7605676511613804\n",
      "\n",
      "\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=0.0,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "<function make_xy at 0x1a2af5d2f0>\n",
      "Naive Bayes Training CLF score: 0.759906194404011\n",
      "Naive Bayes Test CLF score: 0.7632791656560757\n",
      "Logistic Regression AUC: 0.8653941823334219\n",
      "Logistic Regression AUC scores computed using 5-fold cross-validation: [0.82416818 0.87949751 0.86099671 0.88282262 0.88057885]\n",
      "Tuned Decision Tree Parameters: {'criterion': 'entropy', 'max_depth': 3, 'max_features': 6, 'min_samples_leaf': 2}\n",
      "Best Tuned Decision Tree score: 0.7605676511613804\n",
      "\n",
      "\n",
      "<function make_xy_norm at 0x1a2af5dea0>\n",
      "Naive Bayes Training CLF score: 0.7688824195374414\n",
      "Naive Bayes Test CLF score: 0.765704584040747\n",
      "Logistic Regression AUC: 0.8217389336916681\n",
      "Logistic Regression AUC scores computed using 5-fold cross-validation: [0.71394625 0.8065903  0.78292754 0.78824214 0.79311577]\n",
      "Tuned Decision Tree Parameters: {'criterion': 'gini', 'max_depth': None, 'max_features': 2, 'min_samples_leaf': 3}\n",
      "Best Tuned Decision Tree score: 0.7605676511613804\n",
      "\n",
      "\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=0.0,\n",
      "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "<function make_xy at 0x1a2af5d2f0>\n",
      "Naive Bayes Training CLF score: 0.7621704674106421\n",
      "Naive Bayes Test CLF score: 0.7557603686635944\n",
      "Logistic Regression AUC: 0.8792253965502075\n",
      "Logistic Regression AUC scores computed using 5-fold cross-validation: [0.85022426 0.89218147 0.86599843 0.88440741 0.89387673]\n",
      "Tuned Decision Tree Parameters: {'criterion': 'gini', 'max_depth': None, 'max_features': 8, 'min_samples_leaf': 4}\n",
      "Best Tuned Decision Tree score: 0.7608102371277822\n",
      "\n",
      "\n",
      "<function make_xy_norm at 0x1a2af5dea0>\n",
      "Naive Bayes Training CLF score: 0.7607148633349506\n",
      "Naive Bayes Test CLF score: 0.7601261217560029\n",
      "Logistic Regression AUC: 0.8934446750890297\n",
      "Logistic Regression AUC scores computed using 5-fold cross-validation: [0.84456261 0.89504724 0.87577221 0.89238189 0.8972535 ]\n",
      "Tuned Decision Tree Parameters: {'criterion': 'entropy', 'max_depth': None, 'max_features': 2, 'min_samples_leaf': 5}\n",
      "Best Tuned Decision Tree score: 0.7605676511613804\n",
      "\n",
      "\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=0.0,\n",
      "        ngram_range=(2, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "<function make_xy at 0x1a2af5d2f0>\n",
      "Naive Bayes Training CLF score: 0.7621704674106421\n",
      "Naive Bayes Test CLF score: 0.756730536017463\n",
      "Logistic Regression AUC: 0.8638744024337245\n",
      "Logistic Regression AUC scores computed using 5-fold cross-validation: [0.81910943 0.87191115 0.85342196 0.87456919 0.87248662]\n",
      "Tuned Decision Tree Parameters: {'criterion': 'entropy', 'max_depth': 3, 'max_features': 2, 'min_samples_leaf': 3}\n",
      "Best Tuned Decision Tree score: 0.7605676511613804\n",
      "\n",
      "\n",
      "<function make_xy_norm at 0x1a2af5dea0>\n",
      "Naive Bayes Training CLF score: 0.7878052725214297\n",
      "Naive Bayes Test CLF score: 0.7681300024254184\n",
      "Logistic Regression AUC: 0.8046517565426938\n",
      "Logistic Regression AUC scores computed using 5-fold cross-validation: [0.71309009 0.80647977 0.78002291 0.78667958 0.79084572]\n",
      "Tuned Decision Tree Parameters: {'criterion': 'gini', 'max_depth': None, 'max_features': 3, 'min_samples_leaf': 8}\n",
      "Best Tuned Decision Tree score: 0.7605676511613804\n",
      "\n",
      "\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=0.0,\n",
      "        ngram_range=(3, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "<function make_xy at 0x1a2af5d2f0>\n",
      "Naive Bayes Training CLF score: 0.7607957302280446\n",
      "Naive Bayes Test CLF score: 0.765704584040747\n",
      "Logistic Regression AUC: 0.8438797479851865\n",
      "Logistic Regression AUC scores computed using 5-fold cross-validation: [0.76688983 0.83115499 0.82751549 0.84126663 0.82446185]\n",
      "Tuned Decision Tree Parameters: {'criterion': 'entropy', 'max_depth': 3, 'max_features': 4, 'min_samples_leaf': 5}\n",
      "Best Tuned Decision Tree score: 0.7605676511613804\n",
      "\n",
      "\n",
      "<function make_xy_norm at 0x1a2af5dea0>\n",
      "Naive Bayes Training CLF score: 0.8749797832767265\n",
      "Naive Bayes Test CLF score: 0.7557603686635944\n",
      "Logistic Regression AUC: 0.6777085322521081\n",
      "Logistic Regression AUC scores computed using 5-fold cross-validation: [0.61955391 0.66477298 0.64232027 0.67034943 0.654049  ]\n",
      "Tuned Decision Tree Parameters: {'criterion': 'entropy', 'max_depth': 3, 'max_features': 4, 'min_samples_leaf': 6}\n",
      "Best Tuned Decision Tree score: 0.7605676511613804\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizers = [CountVectorizer(), CountVectorizer(ngram_range=(1,2)), CountVectorizer(ngram_range=(2,2)), CountVectorizer(ngram_range=(1,3)), CountVectorizer(ngram_range=(2,3)), CountVectorizer(ngram_range=(3,3)), TfidfVectorizer(min_df=0.), TfidfVectorizer(min_df=0., ngram_range=(1, 2)), TfidfVectorizer(min_df=0., ngram_range=(2, 2)), TfidfVectorizer(min_df=0., ngram_range=(1, 3)), TfidfVectorizer(min_df=0., ngram_range=(2, 3)), TfidfVectorizer(min_df=0., ngram_range=(3, 3)) ]\n",
    "xy_options = [make_xy, make_xy_norm]\n",
    "\n",
    "for vect in vectorizers:\n",
    "    print(str(vect))\n",
    "    for xy in xy_options:\n",
    "        print(str(xy))\n",
    "        run_tests(vect, xy)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression:\n",
    "\n",
    "90%: CountVectorizer(ngram_range=(1, 2), make_xy)\n",
    "\n",
    "90%: CountVectorizer(ngram_range=(1, 3), make_xy)\n",
    "\n",
    "\n",
    "Naive Bayes:\n",
    "\n",
    "81%: CountVectorizer(ngram_range=(1, 1), make_xy) \n",
    "\n",
    "\n",
    "Decision Trees: \n",
    "\n",
    "76%: All cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Parameter Tweaking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayes_tester(X,y):\n",
    "    \n",
    "    # Split into training and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    \n",
    "    #the grid of parameters to search over\n",
    "    alphas = [0.001, 0.01, .1, 1, 5]\n",
    "\n",
    "    #Find the best value for alpha and min_df, and the best classifier\n",
    "    best_alpha = None\n",
    "    best_min_df = None\n",
    "    maxscore=-np.inf\n",
    "    \n",
    "    for alpha in alphas:\n",
    "        clf = MultinomialNB(alpha=alpha)\n",
    "        print('alpha:', alpha)\n",
    "        \n",
    "        # Fit the classifier to the data\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        # Print the accuracy\n",
    "        print(\"Naive Bayes Multinomal Training CLF score:\", clf.score(X_train, y_train))\n",
    "        print(\"Naive Bayes Multinomial Test CLF score:\", clf.score(X_test, y_test)) \n",
    "        \n",
    "        # Compute 5-fold cross-validation scores: cv_scores\n",
    "        cv_scores = cross_val_score(clf, X, y, cv=5)\n",
    "        print('Cross-validation scores:', cv_scores)\n",
    "        print('Mean cross-validation scores:', np.mean(cv_scores))\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bayes_tests(vectorizer, xy_func):\n",
    "    X,y = xy_func(df_adjusted, vectorizer)\n",
    "    bayes_tester(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_df_bayes(xy_func):\n",
    "    min_dfs = [1e-4, 1e-3, 1e-2, 1]\n",
    "    for min_df in min_dfs:\n",
    "        vectorizer = TfidfVectorizer(min_df = min_df)\n",
    "        print('min_df:', min_df)\n",
    "        run_bayes_tests(vectorizer, xy_func)\n",
    "        print('----------------------------------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_df: 0\n",
      "alpha: 1e-05\n",
      "Naive Bayes Multinomal Training CLF score: 0.9712922529516416\n",
      "Naive Bayes Multinomial Test CLF score: 0.8178510793111812\n",
      "\n",
      "\n",
      "[0.78205517 0.79684657 0.797453   0.79860479 0.80800728]\n",
      "0.7965933630626651\n",
      "alpha: 0.01\n",
      "Naive Bayes Multinomal Training CLF score: 0.9564127446223516\n",
      "Naive Bayes Multinomial Test CLF score: 0.8229444579189911\n",
      "\n",
      "\n",
      "[0.78235829 0.80685264 0.80473014 0.81043373 0.80770397]\n",
      "0.80241575375443\n",
      "alpha: 0.1\n",
      "Naive Bayes Multinomal Training CLF score: 0.8692382338670548\n",
      "Naive Bayes Multinomial Test CLF score: 0.7894736842105263\n",
      "\n",
      "\n",
      "[0.77478024 0.78562765 0.78684051 0.7837428  0.78980892]\n",
      "0.7841600225274254\n",
      "alpha: 1\n",
      "Naive Bayes Multinomal Training CLF score: 0.7592592592592593\n",
      "Naive Bayes Multinomial Test CLF score: 0.7652195003638128\n",
      "\n",
      "\n",
      "[0.76023037 0.76015767 0.7607641  0.76008493 0.76099484]\n",
      "0.7604463826195663\n",
      "alpha: 5\n",
      "Naive Bayes Multinomal Training CLF score: 0.7592592592592593\n",
      "Naive Bayes Multinomial Test CLF score: 0.7644918748484113\n",
      "\n",
      "\n",
      "[0.76053349 0.76046089 0.76046089 0.76099484 0.76069154]\n",
      "0.7606283294655283\n",
      "----------------------------------------------------------------------------\n",
      "\n",
      "min_df: 0.0001\n",
      "alpha: 1e-05\n",
      "Naive Bayes Multinomal Training CLF score: 0.955280608119036\n",
      "Naive Bayes Multinomial Test CLF score: 0.7940819791414019\n",
      "\n",
      "\n",
      "[0.78205517 0.79017586 0.79563372 0.79799818 0.80497422]\n",
      "0.7941674297896251\n",
      "alpha: 0.01\n",
      "Naive Bayes Multinomal Training CLF score: 0.9436357755135047\n",
      "Naive Bayes Multinomial Test CLF score: 0.8037836526800873\n",
      "\n",
      "\n",
      "[0.7838739  0.801698   0.80321407 0.80558083 0.80558083]\n",
      "0.7999895262438408\n",
      "alpha: 0.1\n",
      "Naive Bayes Multinomal Training CLF score: 0.891072295002426\n",
      "Naive Bayes Multinomial Test CLF score: 0.7911714770797963\n",
      "\n",
      "\n",
      "[0.78538951 0.79229836 0.80018193 0.80012132 0.80133455]\n",
      "0.7958651344061247\n",
      "alpha: 1\n",
      "Naive Bayes Multinomal Training CLF score: 0.7652434093482129\n",
      "Naive Bayes Multinomial Test CLF score: 0.7479990298326461\n",
      "\n",
      "\n",
      "[0.76023037 0.76015767 0.7607641  0.76038823 0.76160146]\n",
      "0.7606283662410402\n",
      "alpha: 5\n",
      "Naive Bayes Multinomal Training CLF score: 0.764758207989649\n",
      "Naive Bayes Multinomial Test CLF score: 0.7479990298326461\n",
      "\n",
      "\n",
      "[0.76053349 0.76046089 0.76046089 0.76069154 0.76069154]\n",
      "0.7605676682583702\n",
      "----------------------------------------------------------------------------\n",
      "\n",
      "min_df: 0.001\n",
      "alpha: 1e-05\n",
      "Naive Bayes Multinomal Training CLF score: 0.8747371825974446\n",
      "Naive Bayes Multinomial Test CLF score: 0.8142129517341742\n",
      "\n",
      "\n",
      "[0.786602   0.80139478 0.80654942 0.80679406 0.80649075]\n",
      "0.8015662027170262\n",
      "alpha: 0.01\n",
      "Naive Bayes Multinomal Training CLF score: 0.8715833737667799\n",
      "Naive Bayes Multinomial Test CLF score: 0.8149405772495756\n",
      "\n",
      "\n",
      "[0.78690512 0.805943   0.80836871 0.80861389 0.80527753]\n",
      "0.803021648653346\n",
      "alpha: 0.1\n",
      "Naive Bayes Multinomal Training CLF score: 0.8627688824195374\n",
      "Naive Bayes Multinomial Test CLF score: 0.8130002425418384\n",
      "\n",
      "\n",
      "[0.78326766 0.80654942 0.80776228 0.80831059 0.79951471]\n",
      "0.8010809313304341\n",
      "alpha: 1\n",
      "Naive Bayes Multinomal Training CLF score: 0.7831149927219796\n",
      "Naive Bayes Multinomial Test CLF score: 0.7666747513946156\n",
      "\n",
      "\n",
      "[0.76811155 0.76864767 0.77077016 0.76948741 0.77282378]\n",
      "0.7699681139868428\n",
      "alpha: 5\n",
      "Naive Bayes Multinomal Training CLF score: 0.7612809315866085\n",
      "Naive Bayes Multinomial Test CLF score: 0.758428328886733\n",
      "\n",
      "\n",
      "[0.76053349 0.76046089 0.76046089 0.76069154 0.76069154]\n",
      "0.7605676682583702\n",
      "----------------------------------------------------------------------------\n",
      "\n",
      "min_df: 0.01\n",
      "alpha: 1e-05\n",
      "Naive Bayes Multinomal Training CLF score: 0.8010674429888404\n",
      "Naive Bayes Multinomial Test CLF score: 0.8016007761338831\n",
      "\n",
      "\n",
      "[0.77963019 0.78926622 0.78896301 0.79951471 0.78980892]\n",
      "0.7894366096686823\n",
      "alpha: 0.01\n",
      "Naive Bayes Multinomal Training CLF score: 0.8009865760957464\n",
      "Naive Bayes Multinomial Test CLF score: 0.8016007761338831\n",
      "\n",
      "\n",
      "[0.77963019 0.78926622 0.78896301 0.79981802 0.78980892]\n",
      "0.7894972708758404\n",
      "alpha: 0.1\n",
      "Naive Bayes Multinomal Training CLF score: 0.8000970402717128\n",
      "Naive Bayes Multinomial Test CLF score: 0.8008731506184816\n",
      "\n",
      "\n",
      "[0.77963019 0.78896301 0.78865979 0.79951471 0.78920231]\n",
      "0.7891940016267133\n",
      "alpha: 1\n",
      "Naive Bayes Multinomal Training CLF score: 0.791929484069222\n",
      "Naive Bayes Multinomial Test CLF score: 0.7962648556876061\n",
      "\n",
      "\n",
      "[0.7756896  0.78380837 0.78320194 0.79011222 0.78707916]\n",
      "0.7839782596593813\n",
      "alpha: 5\n",
      "Naive Bayes Multinomal Training CLF score: 0.7630600032346757\n",
      "Naive Bayes Multinomial Test CLF score: 0.7756487994178995\n",
      "\n",
      "\n",
      "[0.76417096 0.76470588 0.76440267 0.76554443 0.7688808 ]\n",
      "0.7655409493192338\n",
      "----------------------------------------------------------------------------\n",
      "\n",
      "min_df: 1\n",
      "alpha: 1e-05\n",
      "Naive Bayes Multinomal Training CLF score: 0.9717774543102055\n",
      "Naive Bayes Multinomial Test CLF score: 0.8083919476109629\n",
      "\n",
      "\n",
      "[0.78205517 0.79684657 0.797453   0.79860479 0.80800728]\n",
      "0.7965933630626651\n",
      "alpha: 0.01\n",
      "Naive Bayes Multinomal Training CLF score: 0.9566553453016335\n",
      "Naive Bayes Multinomial Test CLF score: 0.8159107446034441\n",
      "\n",
      "\n",
      "[0.78235829 0.80685264 0.80473014 0.81043373 0.80770397]\n",
      "0.80241575375443\n",
      "alpha: 0.1\n",
      "Naive Bayes Multinomal Training CLF score: 0.8709364386220282\n",
      "Naive Bayes Multinomial Test CLF score: 0.7897162260489935\n",
      "\n",
      "\n",
      "[0.77478024 0.78562765 0.78684051 0.7837428  0.78980892]\n",
      "0.7841600225274254\n",
      "alpha: 1\n",
      "Naive Bayes Multinomal Training CLF score: 0.7612000646935144\n",
      "Naive Bayes Multinomial Test CLF score: 0.7589134125636672\n",
      "\n",
      "\n",
      "[0.76023037 0.76015767 0.7607641  0.76008493 0.76099484]\n",
      "0.7604463826195663\n",
      "alpha: 5\n",
      "Naive Bayes Multinomal Training CLF score: 0.7612809315866085\n",
      "Naive Bayes Multinomial Test CLF score: 0.7586708707252001\n",
      "\n",
      "\n",
      "[0.76053349 0.76046089 0.76046089 0.76099484 0.76069154]\n",
      "0.7606283294655283\n",
      "----------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "min_df_bayes(make_xy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best score: 82%\n",
    "\n",
    "Naive Bayes performs better than Decision Trees (mid 70s), but it does not match Logistic Regression (low 90s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests Round 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOTEFILTER = 10\n",
    "MIDDLEPERCENTAGE = 0.5\n",
    "ROWS = 275000\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Open DataFrames\n",
    "df = pd.read_csv('df.csv', nrows=ROWS)\n",
    "\n",
    "# Minimize dataset\n",
    "df = df[df['Total_Votes']>=VOTEFILTER]\n",
    "\n",
    "# Choose relevant columns\n",
    "df = df[['reviewText', 'overall', 'helpful', 'Review_Length', 'Sentence_Length', 'Word_Length', 'Helpful_Rating', 'Helpful' ]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lose middle rows\n",
    "hand_pick_percentage = False\n",
    "\n",
    "if hand_pick_percentage:\n",
    "    high = 0.85\n",
    "    low = 0.50\n",
    "else:\n",
    "    high = df['Helpful_Rating'].median() + MIDDLEPERCENTAGE*0.25\n",
    "    low = df['Helpful_Rating'].median() - MIDDLEPERCENTAGE*0.75\n",
    "\n",
    "df_adjusted = df[(df['Helpful_Rating']<low) | (df['Helpful_Rating']>high)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reviewText         19686\n",
       "overall            19686\n",
       "helpful            19686\n",
       "Review_Length      19686\n",
       "Sentence_Length    19686\n",
       "Word_Length        19686\n",
       "Helpful_Rating     19686\n",
       "Helpful            19686\n",
       "dtype: int64"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show number of rows\n",
    "df_adjusted.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update corpus\n",
    "corpus = df_adjusted['reviewText']\n",
    "normalize_corpus = np.vectorize(normalize_document)\n",
    "norm_corpus = normalize_corpus(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This test starts with 75000 more rows, a 37.5% increase. After the 10-vote cut, 19,686."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "<function make_xy at 0x1ab669e0d0>\n",
      "Naive Bayes Multinomial Training CLF score: 0.8850739746550971\n",
      "Naive Bayes Multinomial Test CLF score: 0.8215355805243446\n",
      "Cross-validation scores: [0.78469459 0.82518137 0.82720674 0.8243971  0.83306017]\n",
      "Mean cross-validation scores: 0.8189079956951193\n",
      "Logistic Regression AUC: 0.8987001432048829\n",
      "Logistic Regression AUC scores computed using 5-fold cross-validation: [0.86032098 0.87873907 0.88402505 0.90031723 0.9164    ]\n",
      "Tuned Decision Tree Parameters: {'criterion': 'gini', 'max_depth': 3, 'max_features': 4, 'min_samples_leaf': 4}\n",
      "Best Tuned Decision Tree score: 0.7618802378388502\n",
      "\n",
      "\n",
      "<function make_xy_norm at 0x1ab669e158>\n",
      "Naive Bayes Multinomial Training CLF score: 0.8385667020413259\n",
      "Naive Bayes Multinomial Test CLF score: 0.7926966292134832\n",
      "Cross-validation scores: [0.77229113 0.79077931 0.79091548 0.78810583 0.80098338]\n",
      "Mean cross-validation scores: 0.7886150250112561\n",
      "Logistic Regression AUC: 0.9037672835336188\n",
      "Logistic Regression AUC scores computed using 5-fold cross-validation: [0.86156857 0.87885737 0.88225036 0.89894172 0.9157065 ]\n",
      "Tuned Decision Tree Parameters: {'criterion': 'entropy', 'max_depth': None, 'max_features': 1, 'min_samples_leaf': 8}\n",
      "Best Tuned Decision Tree score: 0.761833419167564\n",
      "\n",
      "\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "<function make_xy at 0x1ab669e0d0>\n",
      "Naive Bayes Multinomial Training CLF score: 0.7991759785255009\n",
      "Naive Bayes Multinomial Test CLF score: 0.7707865168539326\n",
      "Cross-validation scores: [0.76386614 0.76456822 0.76679934 0.7658628  0.77054554]\n",
      "Mean cross-validation scores: 0.7663284069908448\n",
      "Logistic Regression AUC: 0.9174078812451936\n",
      "Logistic Regression AUC scores computed using 5-fold cross-validation: [0.87729592 0.89903851 0.90653331 0.91447664 0.92714541]\n",
      "Tuned Decision Tree Parameters: {'criterion': 'entropy', 'max_depth': 3, 'max_features': 5, 'min_samples_leaf': 6}\n",
      "Best Tuned Decision Tree score: 0.761833419167564\n",
      "\n",
      "\n",
      "<function make_xy_norm at 0x1ab669e158>\n",
      "Naive Bayes Multinomial Training CLF score: 0.8742742992696173\n",
      "Naive Bayes Multinomial Test CLF score: 0.7623595505617977\n",
      "Cross-validation scores: [0.76410016 0.76784461 0.76890658 0.76516038 0.7721845 ]\n",
      "Mean cross-validation scores: 0.7676392465680653\n",
      "Logistic Regression AUC: 0.9154821867445252\n",
      "Logistic Regression AUC scores computed using 5-fold cross-validation: [0.86931002 0.89291705 0.89733021 0.91076651 0.92543569]\n",
      "Tuned Decision Tree Parameters: {'criterion': 'entropy', 'max_depth': 3, 'max_features': 1, 'min_samples_leaf': 7}\n",
      "Best Tuned Decision Tree score: 0.761833419167564\n",
      "\n",
      "\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(2, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "<function make_xy at 0x1ab669e0d0>\n",
      "Naive Bayes Multinomial Training CLF score: 0.8735251888382546\n",
      "Naive Bayes Multinomial Test CLF score: 0.7702247191011236\n",
      "Cross-validation scores: [0.76644044 0.77135502 0.77077968 0.77148209 0.78084758]\n",
      "Mean cross-validation scores: 0.7721809603877439\n",
      "Logistic Regression AUC: 0.9039531619023727\n",
      "Logistic Regression AUC scores computed using 5-fold cross-validation: [0.86372333 0.88822305 0.89680865 0.90072033 0.91801392]\n",
      "Tuned Decision Tree Parameters: {'criterion': 'entropy', 'max_depth': 3, 'max_features': 4, 'min_samples_leaf': 5}\n",
      "Best Tuned Decision Tree score: 0.761833419167564\n",
      "\n",
      "\n",
      "<function make_xy_norm at 0x1ab669e158>\n",
      "Naive Bayes Multinomial Training CLF score: 0.9873899744053936\n",
      "Naive Bayes Multinomial Test CLF score: 0.41685393258426967\n",
      "Cross-validation scores: [0.34846712 0.38286918 0.38796535 0.38117537 0.37344884]\n",
      "Mean cross-validation scores: 0.37478517103276204\n",
      "Logistic Regression AUC: 0.8596090150494295\n",
      "Logistic Regression AUC scores computed using 5-fold cross-validation: [0.80034283 0.84500346 0.84337558 0.86241032 0.88842505]\n",
      "Tuned Decision Tree Parameters: {'criterion': 'entropy', 'max_depth': 3, 'max_features': 2, 'min_samples_leaf': 6}\n",
      "Best Tuned Decision Tree score: 0.761833419167564\n",
      "\n",
      "\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "<function make_xy at 0x1ab669e0d0>\n",
      "Naive Bayes Multinomial Training CLF score: 0.8594169423809227\n",
      "Naive Bayes Multinomial Test CLF score: 0.7601123595505618\n",
      "Cross-validation scores: [0.76246197 0.76386614 0.7658628  0.7639897  0.76867244]\n",
      "Mean cross-validation scores: 0.7649706084657724\n",
      "Logistic Regression AUC: 0.9216502570025438\n",
      "Logistic Regression AUC scores computed using 5-fold cross-validation: [0.87916852 0.90078585 0.90860927 0.91521788 0.92852848]\n",
      "Tuned Decision Tree Parameters: {'criterion': 'gini', 'max_depth': None, 'max_features': 8, 'min_samples_leaf': 5}\n",
      "Best Tuned Decision Tree score: 0.761833419167564\n",
      "\n",
      "\n",
      "<function make_xy_norm at 0x1ab669e158>\n",
      "Naive Bayes Multinomial Training CLF score: 0.9843935326799426\n",
      "Naive Bayes Multinomial Test CLF score: 0.7777153558052434\n",
      "Cross-validation scores: [0.76293003 0.78188626 0.77944275 0.77639897 0.78553032]\n",
      "Mean cross-validation scores: 0.7772376664679632\n",
      "Logistic Regression AUC: 0.9088006152179381\n",
      "Logistic Regression AUC scores computed using 5-fold cross-validation: [0.8689512  0.89345091 0.89931732 0.911334   0.92620353]\n",
      "Tuned Decision Tree Parameters: {'criterion': 'entropy', 'max_depth': None, 'max_features': 1, 'min_samples_leaf': 2}\n",
      "Best Tuned Decision Tree score: 0.761833419167564\n",
      "\n",
      "\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(2, 3), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "<function make_xy at 0x1ab669e0d0>\n",
      "Naive Bayes Multinomial Training CLF score: 0.9683500842749235\n",
      "Naive Bayes Multinomial Test CLF score: 0.7702247191011236\n",
      "Cross-validation scores: [0.7406974  0.76714252 0.7721845  0.76750176 0.78787169]\n",
      "Mean cross-validation scores: 0.7670795748138504\n",
      "Logistic Regression AUC: 0.8948814982702726\n",
      "Logistic Regression AUC scores computed using 5-fold cross-validation: [0.86190325 0.88385527 0.89342034 0.89680351 0.91686807]\n",
      "Tuned Decision Tree Parameters: {'criterion': 'gini', 'max_depth': None, 'max_features': 7, 'min_samples_leaf': 6}\n",
      "Best Tuned Decision Tree score: 0.761833419167564\n",
      "\n",
      "\n",
      "<function make_xy_norm at 0x1ab669e158>\n",
      "Naive Bayes Multinomial Training CLF score: 0.9991260378300768\n",
      "Naive Bayes Multinomial Test CLF score: 0.2352059925093633\n",
      "Cross-validation scores: [0.21577346 0.23941025 0.23764926 0.23249824 0.22875205]\n",
      "Mean cross-validation scores: 0.2308166533634579\n",
      "Logistic Regression AUC: 0.8654425644608348\n",
      "Logistic Regression AUC scores computed using 5-fold cross-validation: [0.78990068 0.83437209 0.8383262  0.85588179 0.8855873 ]\n",
      "Tuned Decision Tree Parameters: {'criterion': 'entropy', 'max_depth': 3, 'max_features': 6, 'min_samples_leaf': 3}\n",
      "Best Tuned Decision Tree score: 0.7618802378388502\n",
      "\n",
      "\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(3, 3), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "<function make_xy at 0x1ab669e0d0>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Multinomial Training CLF score: 0.9954429115425433\n",
      "Naive Bayes Multinomial Test CLF score: 0.4265917602996255\n",
      "Cross-validation scores: [0.37280599 0.39925111 0.39030672 0.39967221 0.39662842]\n",
      "Mean cross-validation scores: 0.3917328909292682\n",
      "Logistic Regression AUC: 0.8672628037605213\n",
      "Logistic Regression AUC scores computed using 5-fold cross-validation: [0.82130348 0.84228435 0.8589649  0.86344165 0.89355148]\n",
      "Tuned Decision Tree Parameters: {'criterion': 'gini', 'max_depth': None, 'max_features': 2, 'min_samples_leaf': 5}\n",
      "Best Tuned Decision Tree score: 0.761833419167564\n",
      "\n",
      "\n",
      "<function make_xy_norm at 0x1ab669e158>\n",
      "Naive Bayes Multinomial Training CLF score: 0.9996254447843186\n",
      "Naive Bayes Multinomial Test CLF score: 0.2402621722846442\n",
      "Cross-validation scores: [0.23402762 0.23824011 0.2360103  0.23694685 0.23647858]\n",
      "Mean cross-validation scores: 0.2363406913858495\n",
      "Logistic Regression AUC: 0.7509527535219733\n",
      "Logistic Regression AUC scores computed using 5-fold cross-validation: [0.66511201 0.72728672 0.73911468 0.74938401 0.76998161]\n",
      "Tuned Decision Tree Parameters: {'criterion': 'entropy', 'max_depth': 3, 'max_features': 6, 'min_samples_leaf': 6}\n",
      "Best Tuned Decision Tree score: 0.761833419167564\n",
      "\n",
      "\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=0.0,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "<function make_xy at 0x1ab669e0d0>\n",
      "Naive Bayes Multinomial Training CLF score: 0.7613459017416817\n",
      "Naive Bayes Multinomial Test CLF score: 0.7640449438202247\n",
      "Cross-validation scores: [0.76152586 0.76152586 0.76188246 0.7621166  0.7621166 ]\n",
      "Mean cross-validation scores: 0.7618334767763894\n",
      "Logistic Regression AUC: 0.9093383471831799\n",
      "Logistic Regression AUC scores computed using 5-fold cross-validation: [0.85365872 0.89019191 0.89826333 0.90677989 0.91759148]\n",
      "Tuned Decision Tree Parameters: {'criterion': 'entropy', 'max_depth': 3, 'max_features': 3, 'min_samples_leaf': 8}\n",
      "Best Tuned Decision Tree score: 0.7620675125239945\n",
      "\n",
      "\n",
      "<function make_xy_norm at 0x1ab669e158>\n",
      "Naive Bayes Multinomial Training CLF score: 0.7610961982645609\n",
      "Naive Bayes Multinomial Test CLF score: 0.7653558052434457\n",
      "Cross-validation scores: [0.76175989 0.76175989 0.76188246 0.76188246 0.7621166 ]\n",
      "Mean cross-validation scores: 0.7618802603816124\n",
      "Logistic Regression AUC: 0.8993487809340773\n",
      "Logistic Regression AUC scores computed using 5-fold cross-validation: [0.83660803 0.89008085 0.89709995 0.90619215 0.91540462]\n",
      "Tuned Decision Tree Parameters: {'criterion': 'entropy', 'max_depth': 3, 'max_features': 8, 'min_samples_leaf': 7}\n",
      "Best Tuned Decision Tree score: 0.761833419167564\n",
      "\n",
      "\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=0.0,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "<function make_xy at 0x1ab669e0d0>\n",
      "Naive Bayes Multinomial Training CLF score: 0.7639677882514514\n",
      "Naive Bayes Multinomial Test CLF score: 0.7558052434456929\n",
      "Cross-validation scores: [0.76175989 0.76175989 0.76188246 0.76188246 0.7621166 ]\n",
      "Mean cross-validation scores: 0.7618802603816124\n",
      "Logistic Regression AUC: 0.905038765210538\n",
      "Logistic Regression AUC scores computed using 5-fold cross-validation: [0.85991991 0.88601306 0.89712291 0.9024092  0.92035217]\n",
      "Tuned Decision Tree Parameters: {'criterion': 'gini', 'max_depth': 3, 'max_features': 3, 'min_samples_leaf': 2}\n",
      "Best Tuned Decision Tree score: 0.7620675125239945\n",
      "\n",
      "\n",
      "<function make_xy_norm at 0x1ab669e158>\n",
      "Naive Bayes Multinomial Training CLF score: 0.7625319932580061\n",
      "Naive Bayes Multinomial Test CLF score: 0.7601123595505618\n",
      "Cross-validation scores: [0.76175989 0.76175989 0.76188246 0.76188246 0.76188246]\n",
      "Mean cross-validation scores: 0.761833432940732\n",
      "Logistic Regression AUC: 0.9086981849876692\n",
      "Logistic Regression AUC scores computed using 5-fold cross-validation: [0.8424956  0.8908154  0.89743506 0.90721321 0.92159351]\n",
      "Tuned Decision Tree Parameters: {'criterion': 'gini', 'max_depth': None, 'max_features': 5, 'min_samples_leaf': 2}\n",
      "Best Tuned Decision Tree score: 0.7619738751814223\n",
      "\n",
      "\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=0.0,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "<function make_xy at 0x1ab669e0d0>\n",
      "Naive Bayes Multinomial Training CLF score: 0.7623447156501655\n",
      "Naive Bayes Multinomial Test CLF score: 0.7614232209737828\n",
      "Cross-validation scores: [0.76152586 0.76152586 0.76188246 0.76164833 0.7621166 ]\n",
      "Mean cross-validation scores: 0.7617398218946285\n",
      "Logistic Regression AUC: 0.8918814830029784\n",
      "Logistic Regression AUC scores computed using 5-fold cross-validation: [0.83075305 0.86978202 0.88483065 0.88380174 0.90433346]\n",
      "Tuned Decision Tree Parameters: {'criterion': 'entropy', 'max_depth': None, 'max_features': 3, 'min_samples_leaf': 6}\n",
      "Best Tuned Decision Tree score: 0.761833419167564\n",
      "\n",
      "\n",
      "<function make_xy_norm at 0x1ab669e158>\n",
      "Naive Bayes Multinomial Training CLF score: 0.7687121543167489\n",
      "Naive Bayes Multinomial Test CLF score: 0.7659176029962547\n",
      "Cross-validation scores: [0.76175989 0.76175989 0.76188246 0.76188246 0.7621166 ]\n",
      "Mean cross-validation scores: 0.7618802603816124\n",
      "Logistic Regression AUC: 0.841034582458758\n",
      "Logistic Regression AUC scores computed using 5-fold cross-validation: [0.72280699 0.82402741 0.80252487 0.82093682 0.81247405]\n",
      "Tuned Decision Tree Parameters: {'criterion': 'entropy', 'max_depth': 3, 'max_features': 8, 'min_samples_leaf': 4}\n",
      "Best Tuned Decision Tree score: 0.761833419167564\n",
      "\n",
      "\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=0.0,\n",
      "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "<function make_xy at 0x1ab669e0d0>\n",
      "Naive Bayes Multinomial Training CLF score: 0.7631562519508084\n",
      "Naive Bayes Multinomial Test CLF score: 0.7582397003745318\n",
      "Cross-validation scores: [0.76175989 0.76175989 0.76188246 0.76188246 0.76188246]\n",
      "Mean cross-validation scores: 0.761833432940732\n",
      "Logistic Regression AUC: 0.8887200729866266\n",
      "Logistic Regression AUC scores computed using 5-fold cross-validation: [0.854415   0.87346745 0.88627989 0.89223399 0.91496254]\n",
      "Tuned Decision Tree Parameters: {'criterion': 'entropy', 'max_depth': 3, 'max_features': 7, 'min_samples_leaf': 1}\n",
      "Best Tuned Decision Tree score: 0.761833419167564\n",
      "\n",
      "\n",
      "<function make_xy_norm at 0x1ab669e158>\n",
      "Naive Bayes Multinomial Training CLF score: 0.7610337723952806\n",
      "Naive Bayes Multinomial Test CLF score: 0.7646067415730337\n",
      "Cross-validation scores: [0.76175989 0.76175989 0.76188246 0.76188246 0.76188246]\n",
      "Mean cross-validation scores: 0.761833432940732\n",
      "Logistic Regression AUC: 0.9082520203782647\n",
      "Logistic Regression AUC scores computed using 5-fold cross-validation: [0.84043952 0.8864259  0.89311393 0.90384031 0.92050537]\n",
      "Tuned Decision Tree Parameters: {'criterion': 'gini', 'max_depth': None, 'max_features': 2, 'min_samples_leaf': 3}\n",
      "Best Tuned Decision Tree score: 0.761833419167564\n",
      "\n",
      "\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=0.0,\n",
      "        ngram_range=(2, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "<function make_xy at 0x1ab669e0d0>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Multinomial Training CLF score: 0.7621574380423247\n",
      "Naive Bayes Multinomial Test CLF score: 0.7616104868913858\n",
      "Cross-validation scores: [0.76175989 0.76175989 0.76164833 0.76188246 0.7621166 ]\n",
      "Mean cross-validation scores: 0.7618334329407321\n",
      "Logistic Regression AUC: 0.8880168738576439\n",
      "Logistic Regression AUC scores computed using 5-fold cross-validation: [0.82307497 0.85967848 0.87595148 0.87545833 0.8980095 ]\n",
      "Tuned Decision Tree Parameters: {'criterion': 'entropy', 'max_depth': 3, 'max_features': 7, 'min_samples_leaf': 3}\n",
      "Best Tuned Decision Tree score: 0.7618802378388502\n",
      "\n",
      "\n",
      "<function make_xy_norm at 0x1ab669e158>\n",
      "Naive Bayes Multinomial Training CLF score: 0.7806354953492728\n",
      "Naive Bayes Multinomial Test CLF score: 0.7636704119850187\n",
      "Cross-validation scores: [0.76175989 0.76175989 0.7621166  0.76188246 0.7621166 ]\n",
      "Mean cross-validation scores: 0.7619270878224927\n",
      "Logistic Regression AUC: 0.8169631855196027\n",
      "Logistic Regression AUC scores computed using 5-fold cross-validation: [0.71834596 0.82207907 0.79946775 0.81999101 0.80994392]\n",
      "Tuned Decision Tree Parameters: {'criterion': 'entropy', 'max_depth': 3, 'max_features': 6, 'min_samples_leaf': 8}\n",
      "Best Tuned Decision Tree score: 0.761833419167564\n",
      "\n",
      "\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=0.0,\n",
      "        ngram_range=(3, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "<function make_xy at 0x1ab669e0d0>\n",
      "Naive Bayes Multinomial Training CLF score: 0.7649666021599351\n",
      "Naive Bayes Multinomial Test CLF score: 0.7578651685393258\n",
      "Cross-validation scores: [0.76175989 0.76175989 0.76188246 0.76188246 0.7621166 ]\n",
      "Mean cross-validation scores: 0.7618802603816124\n",
      "Logistic Regression AUC: 0.8458732476540632\n",
      "Logistic Regression AUC scores computed using 5-fold cross-validation: [0.77104289 0.833767   0.84693523 0.8390753  0.85347434]\n",
      "Tuned Decision Tree Parameters: {'criterion': 'entropy', 'max_depth': None, 'max_features': 4, 'min_samples_leaf': 3}\n",
      "Best Tuned Decision Tree score: 0.761833419167564\n",
      "\n",
      "\n",
      "<function make_xy_norm at 0x1ab669e158>\n",
      "Naive Bayes Multinomial Training CLF score: 0.8785816842499532\n",
      "Naive Bayes Multinomial Test CLF score: 0.7704119850187265\n",
      "Cross-validation scores: [0.7610578  0.76152586 0.76164833 0.76094591 0.75977523]\n",
      "Mean cross-validation scores: 0.7609906266762002\n",
      "Logistic Regression AUC: 0.6899482212093984\n",
      "Logistic Regression AUC scores computed using 5-fold cross-validation: [0.60936748 0.67011021 0.66664461 0.68287998 0.67353168]\n",
      "Tuned Decision Tree Parameters: {'criterion': 'gini', 'max_depth': None, 'max_features': 4, 'min_samples_leaf': 7}\n",
      "Best Tuned Decision Tree score: 0.761833419167564\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizers = [CountVectorizer(), CountVectorizer(ngram_range=(1,2)), CountVectorizer(ngram_range=(2,2)), CountVectorizer(ngram_range=(1,3)), CountVectorizer(ngram_range=(2,3)), CountVectorizer(ngram_range=(3,3)), TfidfVectorizer(), TfidfVectorizer(ngram_range=(1, 2)), TfidfVectorizer(ngram_range=(2, 2)), TfidfVectorizer(min_df=0., ngram_range=(1, 3)), TfidfVectorizer(min_df=0., ngram_range=(2, 3)), TfidfVectorizer(ngram_range=(3, 3))]\n",
    "xy_options = [make_xy, make_xy_norm]\n",
    "\n",
    "for vect in vectorizers:\n",
    "    print(str(vect))\n",
    "    for xy in xy_options:\n",
    "        print(str(xy))\n",
    "        run_tests(vect, xy)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression: All close to 90%:\n",
    "\n",
    "(CountVectorizer(ngram_range=(1, 2), make_xy))\n",
    "\n",
    "(CountVectorizer(ngram_range=(1, 2), make_xy_norm))\n",
    "\n",
    "(CountVectorizer(ngram_range=(1, 3), make_xy))\n",
    "\n",
    "(Tfdif(ngram_range=(1, 2), make_xy_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tests are slightly more accurate with more data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updated Tester"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With logistic regression a clear winner, I will now update the tester and seek out more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updated_tester(X, y):\n",
    "    \n",
    "    # Split into training and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    \n",
    "    # LOGISTIC REGRESSION\n",
    "    \n",
    "    # Create the classifier: logreg\n",
    "    logreg = LogisticRegression()\n",
    "\n",
    "    # Fit the classifier to the training data\n",
    "    logreg.fit(X_train, y_train)\n",
    "    \n",
    "    # Compute predicted probabilities: y_pred_prob\n",
    "    y_pred_prob = logreg.predict_proba(X_test)[:,1]\n",
    "\n",
    "    # Compute and print AUC score\n",
    "    print(\"Logistic Regression AUC: {}\".format(roc_auc_score(y_test, y_pred_prob)))\n",
    "\n",
    "    # Compute cross-validated AUC scores: cv_auc\n",
    "    cv_auc = cross_val_score(logreg, X, y, cv=5, scoring='roc_auc')\n",
    "\n",
    "    # Print list of AUC scores\n",
    "    print(\"AUC scores computed using 5-fold cross-validation: {}\".format(cv_auc))\n",
    "    \n",
    "    print(\"Logistic Regression AUC cross-validation mean:\", np.mean(cv_auc))\n",
    "    \n",
    "    # Predict the labels of the test set: y_pred\n",
    "    y_pred = logreg.predict(X_test)\n",
    "\n",
    "    # Compute and print the confusion matrix and classification report\n",
    "    print('Confusion Matrix:', confusion_matrix(y_test, y_pred))\n",
    "    print('Classification Report:', classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_updated_tests(vectorizer, xy_func):\n",
    "    X,y = xy_func(df_adjusted, vectorizer)\n",
    "    updated_tester(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Round 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "<function make_xy at 0x1ab669e0d0>\n",
      "Logistic Regression AUC: 0.9050707174056077\n",
      "Logistic Regression AUC cross-validation: [0.86032098 0.87873907 0.88402505 0.90031723 0.9164    ]\n",
      "Logistic Regression AUC cross-valication mean: 0.8879604640868543\n",
      "Confusion Matrix: [[ 944  354]\n",
      " [ 348 3694]]\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      0.73      0.73      1298\n",
      "          1       0.91      0.91      0.91      4042\n",
      "\n",
      "avg / total       0.87      0.87      0.87      5340\n",
      "\n",
      "\n",
      "\n",
      "<function make_xy_norm at 0x1ab669e158>\n",
      "Logistic Regression AUC: 0.9066543008663835\n",
      "Logistic Regression AUC cross-validation: [0.86156857 0.87885737 0.88225036 0.89894172 0.9157065 ]\n",
      "Logistic Regression AUC cross-valication mean: 0.8874649036672876\n",
      "Confusion Matrix: [[ 962  330]\n",
      " [ 355 3693]]\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      0.74      0.74      1292\n",
      "          1       0.92      0.91      0.92      4048\n",
      "\n",
      "avg / total       0.87      0.87      0.87      5340\n",
      "\n",
      "\n",
      "\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "<function make_xy at 0x1ab669e0d0>\n",
      "Logistic Regression AUC: 0.9118218520325236\n",
      "Logistic Regression AUC cross-validation: [0.87729592 0.89903851 0.90653331 0.91447664 0.92714541]\n",
      "Logistic Regression AUC cross-valication mean: 0.9048979577858685\n",
      "Confusion Matrix: [[ 945  332]\n",
      " [ 301 3762]]\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.74      0.75      1277\n",
      "          1       0.92      0.93      0.92      4063\n",
      "\n",
      "avg / total       0.88      0.88      0.88      5340\n",
      "\n",
      "\n",
      "\n",
      "<function make_xy_norm at 0x1ab669e158>\n",
      "Logistic Regression AUC: 0.9081929715117489\n",
      "Logistic Regression AUC cross-validation: [0.86931002 0.89291705 0.89733021 0.91076651 0.92543569]\n",
      "Logistic Regression AUC cross-valication mean: 0.8991518957049724\n",
      "Confusion Matrix: [[ 924  294]\n",
      " [ 314 3808]]\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.76      0.75      1218\n",
      "          1       0.93      0.92      0.93      4122\n",
      "\n",
      "avg / total       0.89      0.89      0.89      5340\n",
      "\n",
      "\n",
      "\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(2, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "<function make_xy at 0x1ab669e0d0>\n",
      "Logistic Regression AUC: 0.9054650395351546\n",
      "Logistic Regression AUC cross-validation: [0.86372333 0.88822305 0.89680865 0.90072033 0.91801392]\n",
      "Logistic Regression AUC cross-valication mean: 0.8934978557558674\n",
      "Confusion Matrix: [[ 890  383]\n",
      " [ 314 3753]]\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      0.70      0.72      1273\n",
      "          1       0.91      0.92      0.92      4067\n",
      "\n",
      "avg / total       0.87      0.87      0.87      5340\n",
      "\n",
      "\n",
      "\n",
      "<function make_xy_norm at 0x1ab669e158>\n",
      "Logistic Regression AUC: 0.872161797469394\n",
      "Logistic Regression AUC cross-validation: [0.80034283 0.84500346 0.84337558 0.86241032 0.88842505]\n",
      "Logistic Regression AUC cross-valication mean: 0.8479114476557396\n",
      "Confusion Matrix: [[ 818  441]\n",
      " [ 405 3676]]\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.67      0.65      0.66      1259\n",
      "          1       0.89      0.90      0.90      4081\n",
      "\n",
      "avg / total       0.84      0.84      0.84      5340\n",
      "\n",
      "\n",
      "\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "<function make_xy at 0x1ab669e0d0>\n",
      "Logistic Regression AUC: 0.9169139702068327\n",
      "Logistic Regression AUC cross-validation: [0.87916852 0.90078585 0.90860927 0.91521788 0.92852848]\n",
      "Logistic Regression AUC cross-valication mean: 0.9064619982082414\n",
      "Confusion Matrix: [[ 949  360]\n",
      " [ 277 3754]]\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.72      0.75      1309\n",
      "          1       0.91      0.93      0.92      4031\n",
      "\n",
      "avg / total       0.88      0.88      0.88      5340\n",
      "\n",
      "\n",
      "\n",
      "<function make_xy_norm at 0x1ab669e158>\n",
      "Logistic Regression AUC: 0.9119446633035098\n",
      "Logistic Regression AUC cross-validation: [0.8689512  0.89345091 0.89931732 0.911334   0.92620353]\n",
      "Logistic Regression AUC cross-valication mean: 0.8998513904191725\n",
      "Confusion Matrix: [[ 984  337]\n",
      " [ 298 3721]]\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.74      0.76      1321\n",
      "          1       0.92      0.93      0.92      4019\n",
      "\n",
      "avg / total       0.88      0.88      0.88      5340\n",
      "\n",
      "\n",
      "\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(2, 3), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "<function make_xy at 0x1ab669e0d0>\n",
      "Logistic Regression AUC: 0.8881997398336373\n",
      "Logistic Regression AUC cross-validation: [0.86190325 0.88385527 0.89342034 0.89680351 0.91686807]\n",
      "Logistic Regression AUC cross-valication mean: 0.8905700884332912\n",
      "Confusion Matrix: [[ 897  386]\n",
      " [ 327 3730]]\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      0.70      0.72      1283\n",
      "          1       0.91      0.92      0.91      4057\n",
      "\n",
      "avg / total       0.86      0.87      0.87      5340\n",
      "\n",
      "\n",
      "\n",
      "<function make_xy_norm at 0x1ab669e158>\n",
      "Logistic Regression AUC: 0.8575535509428902\n",
      "Logistic Regression AUC cross-validation: [0.78990068 0.83437209 0.8383262  0.85588179 0.8855873 ]\n",
      "Logistic Regression AUC cross-valication mean: 0.8408136121543353\n",
      "Confusion Matrix: [[ 785  461]\n",
      " [ 483 3611]]\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.62      0.63      0.62      1246\n",
      "          1       0.89      0.88      0.88      4094\n",
      "\n",
      "avg / total       0.82      0.82      0.82      5340\n",
      "\n",
      "\n",
      "\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(3, 3), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "<function make_xy at 0x1ab669e0d0>\n",
      "Logistic Regression AUC: 0.8610450956399138\n",
      "Logistic Regression AUC cross-validation: [0.82130348 0.84228435 0.8589649  0.86344165 0.89355148]\n",
      "Logistic Regression AUC cross-valication mean: 0.855909172390635\n",
      "Confusion Matrix: [[ 795  447]\n",
      " [ 366 3732]]\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.68      0.64      0.66      1242\n",
      "          1       0.89      0.91      0.90      4098\n",
      "\n",
      "avg / total       0.84      0.85      0.85      5340\n",
      "\n",
      "\n",
      "\n",
      "<function make_xy_norm at 0x1ab669e158>\n",
      "Logistic Regression AUC: 0.7568802452778711\n",
      "Logistic Regression AUC cross-validation: [0.66511201 0.72728672 0.73911468 0.74938401 0.76998161]\n",
      "Logistic Regression AUC cross-valication mean: 0.7301758049625583\n",
      "Confusion Matrix: [[ 108 1211]\n",
      " [  56 3965]]\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.66      0.08      0.15      1319\n",
      "          1       0.77      0.99      0.86      4021\n",
      "\n",
      "avg / total       0.74      0.76      0.69      5340\n",
      "\n",
      "\n",
      "\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=0.0,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "<function make_xy at 0x1ab669e0d0>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression AUC: 0.9046066231826821\n",
      "Logistic Regression AUC cross-validation: [0.85365872 0.89019191 0.89826333 0.90677989 0.91759148]\n",
      "Logistic Regression AUC cross-valication mean: 0.8932970642362156\n",
      "Confusion Matrix: [[ 764  512]\n",
      " [ 152 3912]]\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.60      0.70      1276\n",
      "          1       0.88      0.96      0.92      4064\n",
      "\n",
      "avg / total       0.87      0.88      0.87      5340\n",
      "\n",
      "\n",
      "\n",
      "<function make_xy_norm at 0x1ab669e158>\n",
      "Logistic Regression AUC: 0.9073250119799385\n",
      "Logistic Regression AUC cross-validation: [0.83660803 0.89008085 0.89709995 0.90619215 0.91540462]\n",
      "Logistic Regression AUC cross-valication mean: 0.889077120126396\n",
      "Confusion Matrix: [[ 587  647]\n",
      " [  95 4011]]\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.48      0.61      1234\n",
      "          1       0.86      0.98      0.92      4106\n",
      "\n",
      "avg / total       0.86      0.86      0.85      5340\n",
      "\n",
      "\n",
      "\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=0.0,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "<function make_xy at 0x1ab669e0d0>\n",
      "Logistic Regression AUC: 0.9050107144050282\n",
      "Logistic Regression AUC cross-validation: [0.85991991 0.88601306 0.89712291 0.9024092  0.92035217]\n",
      "Logistic Regression AUC cross-valication mean: 0.8931634478397614\n",
      "Confusion Matrix: [[ 690  605]\n",
      " [ 115 3930]]\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.53      0.66      1295\n",
      "          1       0.87      0.97      0.92      4045\n",
      "\n",
      "avg / total       0.86      0.87      0.85      5340\n",
      "\n",
      "\n",
      "\n",
      "<function make_xy_norm at 0x1ab669e158>\n",
      "Logistic Regression AUC: 0.9098188997821349\n",
      "Logistic Regression AUC cross-validation: [0.8424956  0.8908154  0.89743506 0.90721321 0.92159351]\n",
      "Logistic Regression AUC cross-valication mean: 0.8919105565197105\n",
      "Confusion Matrix: [[ 289  971]\n",
      " [  27 4053]]\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.23      0.37      1260\n",
      "          1       0.81      0.99      0.89      4080\n",
      "\n",
      "avg / total       0.83      0.81      0.77      5340\n",
      "\n",
      "\n",
      "\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=0.0,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "<function make_xy at 0x1ab669e0d0>\n",
      "Logistic Regression AUC: 0.8836098199732003\n",
      "Logistic Regression AUC cross-validation: [0.83075305 0.86978202 0.88483065 0.88380174 0.90433346]\n",
      "Logistic Regression AUC cross-valication mean: 0.8747001837952301\n",
      "Confusion Matrix: [[ 235 1002]\n",
      " [  24 4079]]\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.19      0.31      1237\n",
      "          1       0.80      0.99      0.89      4103\n",
      "\n",
      "avg / total       0.83      0.81      0.76      5340\n",
      "\n",
      "\n",
      "\n",
      "<function make_xy_norm at 0x1ab669e158>\n",
      "Logistic Regression AUC: 0.8293708957202147\n",
      "Logistic Regression AUC cross-validation: [0.72280699 0.82402741 0.80252487 0.82093682 0.81247405]\n",
      "Logistic Regression AUC cross-valication mean: 0.7965540285607215\n",
      "Confusion Matrix: [[   2 1245]\n",
      " [   0 4093]]\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.00      0.00      1247\n",
      "          1       0.77      1.00      0.87      4093\n",
      "\n",
      "avg / total       0.82      0.77      0.67      5340\n",
      "\n",
      "\n",
      "\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=0.0,\n",
      "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "<function make_xy at 0x1ab669e0d0>\n",
      "Logistic Regression AUC: 0.889969814743474\n",
      "Logistic Regression AUC cross-validation: [0.854415   0.87346745 0.88627989 0.89223399 0.91496254]\n",
      "Logistic Regression AUC cross-valication mean: 0.8842717735910096\n",
      "Confusion Matrix: [[ 587  668]\n",
      " [  89 3996]]\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.47      0.61      1255\n",
      "          1       0.86      0.98      0.91      4085\n",
      "\n",
      "avg / total       0.86      0.86      0.84      5340\n",
      "\n",
      "\n",
      "\n",
      "<function make_xy_norm at 0x1ab669e158>\n",
      "Logistic Regression AUC: 0.9021104234382826\n",
      "Logistic Regression AUC cross-validation: [0.84043952 0.8864259  0.89311393 0.90384031 0.92050537]\n",
      "Logistic Regression AUC cross-valication mean: 0.8888650066816833\n",
      "Confusion Matrix: [[ 106 1157]\n",
      " [   7 4070]]\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.08      0.15      1263\n",
      "          1       0.78      1.00      0.87      4077\n",
      "\n",
      "avg / total       0.82      0.78      0.70      5340\n",
      "\n",
      "\n",
      "\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=0.0,\n",
      "        ngram_range=(2, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "<function make_xy at 0x1ab669e0d0>\n",
      "Logistic Regression AUC: 0.8855909912694394\n",
      "Logistic Regression AUC cross-validation: [0.82307497 0.85967848 0.87595148 0.87545833 0.8980095 ]\n",
      "Logistic Regression AUC cross-valication mean: 0.8664345510021857\n",
      "Confusion Matrix: [[  36 1207]\n",
      " [   5 4092]]\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.03      0.06      1243\n",
      "          1       0.77      1.00      0.87      4097\n",
      "\n",
      "avg / total       0.80      0.77      0.68      5340\n",
      "\n",
      "\n",
      "\n",
      "<function make_xy_norm at 0x1ab669e158>\n",
      "Logistic Regression AUC: 0.8274429652457626\n",
      "Logistic Regression AUC cross-validation: [0.71834596 0.82207907 0.79946775 0.81999101 0.80994392]\n",
      "Logistic Regression AUC cross-valication mean: 0.7939655424011722\n",
      "Confusion Matrix: [[   0 1256]\n",
      " [   0 4084]]\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00      1256\n",
      "          1       0.76      1.00      0.87      4084\n",
      "\n",
      "avg / total       0.58      0.76      0.66      5340\n",
      "\n",
      "\n",
      "\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=0.0,\n",
      "        ngram_range=(3, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "<function make_xy at 0x1ab669e0d0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression AUC: 0.8544884923034963\n",
      "Logistic Regression AUC cross-validation: [0.77104289 0.833767   0.84693523 0.8390753  0.85347434]\n",
      "Logistic Regression AUC cross-valication mean: 0.8288589523372677\n",
      "Confusion Matrix: [[   0 1321]\n",
      " [   0 4019]]\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00      1321\n",
      "          1       0.75      1.00      0.86      4019\n",
      "\n",
      "avg / total       0.57      0.75      0.65      5340\n",
      "\n",
      "\n",
      "\n",
      "<function make_xy_norm at 0x1ab669e158>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression AUC: 0.7000684390093352\n",
      "Logistic Regression AUC cross-validation: [0.60936748 0.67011021 0.66664461 0.68287998 0.67353168]\n",
      "Logistic Regression AUC cross-valication mean: 0.6605067915069508\n",
      "Confusion Matrix: [[   0 1249]\n",
      " [   0 4091]]\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00      1249\n",
      "          1       0.77      1.00      0.87      4091\n",
      "\n",
      "avg / total       0.59      0.77      0.66      5340\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "for vect in vectorizers:\n",
    "    print(str(vect))\n",
    "    for xy in xy_options:\n",
    "        print(str(xy))\n",
    "        run_updated_tests(vect, xy)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression Cross-Validation Means over 0.9:\n",
    "\n",
    "mean: 0.9064619982082414: CV((1,3), make_xy)\n",
    "\n",
    "mean: 0.9048979577858685: CV((1,2), make_xy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix reveals that reviews predicted as unhelpful(0) are usually in the 70th percentile, whereas reviews predicted as helpful(1) are usually in the 90th percentile.\n",
    "\n",
    "C(1,2) gives highest percentage of correct 0 classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation of Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of surprises. Firstly, CV usually outperforms Tfidf, and secondly, make_xy usually outperforms make_xy_norm.\n",
    "\n",
    "The confusion matrix reveals that more attention should be paid to the classification of unhelpful reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust min_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression has a min_df paramter that can be tuned to obtain better results. When greater than 0, it discounts the percentage of frequency of words. For instance, df_min - 0.01 would discount words that appear in less than 1% of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from six.moves import range\n",
    "\n",
    "# Setup Seaborn\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Cumulative Frequency function\n",
    "def ecdf(data):\n",
    "    \"\"\"Compute ECDF for a one-dimensional array of measurements.\"\"\"\n",
    "\n",
    "    # Number of data points: n\n",
    "    n = len(data)\n",
    "\n",
    "    # x-data for the ECDF: x\n",
    "    x = np.sort(data)\n",
    "\n",
    "    # y-data for the ECDF: y\n",
    "    y = np.arange(1, n+1) / n\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn X (df.reviewText) into one dimensional array\n",
    "dfg = list(sorted((X > 0).sum(axis=0).reshape(-1).tolist()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'ECDF')"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a532b6c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute ECDF\n",
    "x1, y1 = ecdf(dfg)\n",
    "\n",
    "# Generate plot\n",
    "plt.plot(x1, y1, marker='.', linestyle='none')\n",
    "\n",
    "# Make the margins nice\n",
    "plt.margins(.02)\n",
    "\n",
    "# Label the axes\n",
    "plt.xlabel('Word Count')\n",
    "plt.ylabel('ECDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 80)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZoAAAEWCAYAAABfdFHAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAHtFJREFUeJzt3Xm4XFWZ7/HvMQRsCYhIvICMCc1rbNDWiHSLEwqCHaRR2m6cIN4L4gAok4JXRicEUbEBEcVWxOHaqDSaQBBoJwYVbJz6+BIItDiBAoFIIkbI/WPtCkVZJ6eqTu060/fzPHl2au21q1bWk1O/s/Zee+2hNWvWIElSXR4z3g2QJE1tBo0kqVYGjSSpVgaNJKlWBo0kqVYGjSSpVuuN54dHxL7A5zJzo1Hq7QScBewK3AOcA5yemc7NlqQJbtyCJiKeA1wEDI1S70nAlcBPgX8Gngm8F3gI+GC3n3vjjTcaTpLUpfnz56/zu3pdBh40EbEB8Fbg3cADwPqjHPIWSjv3zcyVwOLqPY6PiLMyc3W3bZg/f363h6jF8PAwAPPmzRvnlkwN9md/2Z/9Mzw8zMqVK8f0HuNxjealwPHAscC/dlB/D+CqKmQaLgE2BXbpf/MkSf00HqfOfgBsn5nLI+LkDurvCHyzpWxZ075ru21A47cd9W7VqlWAfdkv9md/2Z/90+jLsRh40GTmr7o8ZGNgRUvZiqZ9kqQJbFxnnXVoCBjpAv7Dvbyh523HznPg/WV/9pf92T+T9RpNt+4DWqc/b9S0T5I0gU2GEc1SYE5LWeN1DrgtklSbt37xRi7/yZ3svfP/4qwD5rPdcYvW7rv9tAWjvgb46+MWsRqYCSxtU2enExfxhz/BrPXhp6eO/p77fmYZX3zl5mP6dw2N5/NoqskAx2TmrHXUeTdwKGUCwQNNZW8CtszMP3XzmTfeeOMapzePnacm+sv+7K9uv6A7+QKv+z0nsi+/cvMx3Ucz4YImIuYCszPz+ur1FsAw8CPgDODpwCnAcZnZ0w2bBs3Y+cXYX5OlP/3Cnp6mYtB8GjgoM4eayp5FWYJmPnAncG5mfqCXzzRo+mOyfDFOFqP15/bHLWINZWbMbQP6wm6to+lrUgfNeDBo+sOgGVm/f4uXxptB0yWDpj+mUtD0c2QgTUVjDZrJML1Z6th2xy1a+6fT163Hj+W1VJfG6Hhdr28/bQEzq9czR6gzq1pdctb6nb3nTMbOEY16MogRjdcQ1A8bPXY9Vvzxz2tfT9RJDCPVGW+NGzY9ddYFg6Y/egkaT0FNbRPpC/vMS65nUd7PwS/ckVfvul0f/nXTl0HTA4OmP9oFjcExOP34gu4lBOKdi3jwYdjgMZDvmxi/cbczla4hjjeDpgcGTW8cjfSmzt/iNTKDpn8Mmh4YNO0ZHO0NKgT8Yuwv+7N/+hE0k2GtM9VgXUEyVUOm09NF7Y7r5rWkRzNopoGpMlrpZDTRuihhu/fopExS/xg0U9BkHK20C43nfeBK7rj3QbZ+wgZ85x17rC1vPa7ZWQfMhwPqb6+kzhk0U8BEDY9mnVzLaC1rhIukyc2gmYQmWrD0eq1D0vRg0EwC4x0s7UYj7Wb1GCSS2jFoJqCJECydlElSJwyaCWYQIeMNgJIGyaAZZ4MevTRCxXCRNCgGzYANIlj+aj1Y9eeyHX6PgSJpfBk0AzSo02KSNJEYNDXb4bhF/Hn0aj0zWCRNdAZNjeoYwRgskiYbg6Ymc/sYMoaLpMnsMePdgKmm8Tz6h3o8/vbTFjB/m8czBMzf5vGGjKRJzxFNH431VFkjVL785uf2ozmSNCEYNH3Sa8g4YpE01Rk0YzSWUYwhI2k6MGjGwFGMJI3OyQA9MmQkqTMGzQAZMpKmI0+dDYABI2k6G5egiYhDgLcDWwE3AUdl5nXrqP8c4HTg6cDvgM8A78vM1QNo7lq9nC4zZCRNdwM/dRYRBwLnARcB+wPLgSURsf0I9ecCVwB/qOp/GHgH8P6BNLhiyEhSbwY6oomIIeBU4PzMPKUq+waQwJHAEW0O+ydgBrB/Zj4AXBERWwCHRcSxmbmm7nZ3GzIGjCQ9YtAjmh2AbYFLGwXV6a9FwN4jHLMBsBpY1VR2NzCr2idJmsAGfY1mx2p7S0v5MmBuRMzIzNZlwj5HGe28PyI+AMwF3gZ8NTP/2EsjhoeHezlswrz/RLBqVcn96fBvHQT7s7/sz/5p9OVYDHpEs3G1XdFSvqJqy4atB2TmrcAx1Z+7ge8DdwGvr6+ZvbvsoDnj3QRJmlAGPaIZqrat11Ua5Q+3HhARBwOfAM4H/h+wJeU6z6KI2CMzH+y2EfPmzeuontdmRtb4TbHTvtS62Z/9ZX/2z/DwMCtXrhzTeww6aO6rthsBdzaVz6KEzANtjjkOWJyZhzYKIuIGYBh4DfCpOhraachMp3CRpF4M+tTZ0mrben5pDpAjzCDbGri+uSAzf045jfbUvreQep6MKUnT1XgEzR3Afo2CiJgJLACuGuGYm4HdmgsiYgfgicBt9TRTktQvAz11lplrIuI04OyIuBe4BjgM2IxyI2bjBs3ZmdkYxZwKfCkiPgl8AdgcOBm4HbhwkO2XJHVv4CsDZOa5wLHA64CLgU2AvTJzWVXlBOC6pvr/TlkR4JnAYsqKAN8Gds3M1tlrA+X1GUka3bisdZaZZwJnjrBvIbCwpewrwFdqb1gXDBlJ6oyrN3fJgJGk7vg8GklSrQwaSVKtPHXWxPtnJKn/HNFUDBlJqodBgyEjSXUyaCRJtTJoJEm1Mmi64D00ktQ9g6ZDhowk9cbpzaMwYCRpbBzRSJJqZdBIkmpl0EiSamXQSJJqZdBIkmpl0EiSamXQSJJqZdBIkmpl0EiSamXQSJJqZdBIkmpl0EiSamXQSJJqNa1Xb/YRzpJUv2k7ojFkJGkwpmXQGDKSNDjTMmgkSYNj0EiSajUukwEi4hDg7cBWwE3AUZl53TrqzwbOBPahhOO3gbdl5rI62+ljnCVp7AY+oomIA4HzgIuA/YHlwJKI2H6E+jOBbwDPBg4BFgJzgcsiYv262mnISFJ/DHREExFDwKnA+Zl5SlX2DSCBI4Ej2hx2ILAj8JTM/EV1zO3AYmBn4MZ+ttGAkaT+GvSpsx2AbYFLGwWZuToiFgF7j3DMy4HLGyFTHXMTsGWdDZUk9cegg2bHantLS/kyYG5EzMjMh1r2PQ24KCJOAt4EPAG4EnhTc/j0y/DwcL/fckpatWoVYH/1i/3ZX/Zn/zT6ciwGfY1m42q7oqV8RdWWDdscMxt4PWXE83+A1wFPBRZFxLRe2UCSJoNBf1EPVds1I5Q/3OaYmcD6wEszczlARCwDfgC8AvhSPxs4b968fr7dlNX4TdH+6g/7s7/sz/4ZHh5m5cqVY3qPUUc0EXFXROwypk95xH3VdqOW8lmUkHmgzTF/AL7XCBmAzLyBMltt5z61S5JUk05GNJtRRhUARMQM4E/ALpn5wy4/b2m1ncOjr9PMATIzW0c6VPXaTWNej78cGUmSJpher9EMjV6lraXAHcB+jYLqPpkFwFUjHHMFsFtEbNl0zAsoo6Bre2yHJGlABnqNJjPXRMRpwNkRcS9wDXAYZdT0YYCImAvMzszrq8M+DPxvyg2aJwGPA86ghMwVg2y/JKl7A18ZIDPPBY6lzB67GNgE2KtpOZkTgOua6v8O2A24DfgscDZlpYAFmdlu8oAkaQIZl+nBmXkmZe2ydvsWUpaZaS67labTbZKkyaPToHlCRDyp5ZhNm8rWysy7+tIySdKU0GnQXNqmbMkIdWf02BZJ0hTUSdCcUnsrJElT1qhB01hlWZKkXnQ9GaBa6v+JlJsl7xnhJktJkoAupjdHxL7Vs2NWAncCdwH3R8TXImLPuhooSZrcOgqaiDgX+CrwPMpilhdXf34M7AFcHhFn1NVISdLkNeqps4h4NfBG4JPA8Zl5d8v+TSkTBo6KiGsy85JaWipJmpQ6uUZzCPD1zHxDu52ZeQ9weERsAxwKGDSSpLU6OXX2NODzHdT7EvDMsTVHkjTVdBI0G1Mu/o/mV8CmY2uOJGmq6SRoZgAPdlBvdYfvJ0maRgwGSVKtOr1h80URsdUodZ4y1sZIkqaeToPm1A7ruUqAJOlROgma3WtvhSRpyupkUc1vDaIhkqSpqZu1zjaNiLltyo9u9wA0SZKg87XO9gNuo9z531y+LXAGcGtEvKT/zZMkTXajBk1E7Ax8EbgV+FrL7juABcAy4D8i4q/73kJJ0qTWyYjmWOBm4LmZ+Z3mHZn5cGZeBjyXsjLAO/rfREnSZNZJ0OwGfDQzV45UITNXAP9KeYyAJElrdRI0W1BOjY3mv4Gtx9YcSdJU00nQ3A10MqtsE2D52JojSZpqOgmaG4BXdFDv5ZRrOZIkrdVJ0PwbsH9ELBypQkQcBPwL8Nk+tUuSNEV0sjLApRFxMXBBRLyK8gTNW6tj5wD7UZapuZISSpIkrdXpopqvBm4HDgf25JHFM4eAPwJnAidm5sP9bqAkaXLrKGgy8yHgHRHxPuAlwDbAQ5TVAq7KzD/U10RJ0mQ2atBExBuASzLzrsy8D/j3EerNAz6Qmft28J6HAG8HtgJuAo7KzOs6aXBEnAyclJlDndSXJI2vTiYDfAzYofEiIh4TEfdExNNa6m1KWY5mnSLiQOA84CJgf8qU6CURsX0Hx+4EHN9BmyVJE0QnQdM6chii3DPT6fWdtSJiiPIQtfMz85TMXAzsC/weOHKUY2cAFwC/6/ZzJUnjp+PHBPTJDsC2wKWNgsxcDSwC9h7l2COBjSlL3UiSJomuRyVjtGO1vaWlfBkwNyJmVBMPHiUidgBOpoTRs+ps4PDwcJ1vP2WsWrUKsL/6xf7sL/uzfxp9ORaDHtFsXG1XtJSvqNqyYesB1em2TwKfzczv1ts8SVK/DXpE07jes2aE8nb34RxKOeU26my2fpg3b94gPmbSa/ymaH/1h/3ZX/Zn/wwPD7Ny5YiL93ek0xFNazCMVDaa+6rtRi3lsygh80BzYURsDZwOvBVYGRHrUbU5ItaLiEGPyCRJXep0RHN1RLQGy7UtZZ3c17K02s7h0ddp5gCZma2f8WJKKF3c5r1WA6dQrt1IkiaoToLmM338vKWUxz/vB1wBEBEzKfffLGpT/2vALi1lrwKOqsp/3ce2SZJq0Mmimq/v14dl5pqIOA04OyLuBa4BDgM2Az4MEBFzgdmZeX1m3k15Hs5aEfHc6r1u6Fe7JEn1Gfg1jsw8FzgWeB3llNgmwF6Z2XiK5wlAR8vRSJImvkHPOgMgM8+krPjcbt9CYOE6jv0I8JFaGiZJ6jtnbUmSamXQSJJqZdBIkmpl0EiSamXQSJJqZdBIkmpl0EiSamXQSJJqZdBIkmpl0EiSamXQSJJqZdBIkmpl0EiSamXQSJJqZdBIkmpl0EiSamXQSJJqZdBIkmpl0EiSamXQSJJqZdBIkmpl0EiSamXQSJJqZdBIkmpl0EiSamXQSJJqZdBIkmpl0EiSarXeeHxoRBwCvB3YCrgJOCozr1tH/ecA7wWeAawErgSOzcw7B9BcSdIYDHxEExEHAucBFwH7A8uBJRGx/Qj15wFXASuAVwHHALtVx8wcSKMlST0b6IgmIoaAU4HzM/OUquwbQAJHAke0Oeww4DfA/pm5ujpmKfB9YE9g8QCaLknq0aBHNDsA2wKXNgqq8FgE7D3CMT8DzmyETOOwatt2FCRJmjgGfY1mx2p7S0v5MmBuRMzIzIead2TmuW3e52XV9ud9bp8kqc8GHTQbV9sVLeUrKKOrDYH71/UGEbE18EHgBuDqfjdweHi43285Ja1atQqwv/rF/uwv+7N/Gn05FoM+dTZUbdeMUP7wug6uQuYqSrsPyMzW95EkTTCDHtHcV203ApqnJs+ihMwDIx0YETsBlwEzgT0z89Y6Gjhv3rw63nbKafymaH/1h/3ZX/Zn/wwPD7Ny5coxvcegRzRLq+2clvI5QI40QomIXYFvAw8Bz8vMH9fXRElSP41H0NwB7NcoqO6FWUA5JfYXImI7ykjmTuA5mbm0XT1J0sQ00FNnmbkmIk4Dzo6Ie4FrKPfJbAZ8GCAi5gKzM/P66rCzKJMI3gJsExHbNL3l/2Tmbwb2D5AkdW3gKwNU05WPBV4HXAxsAuyVmcuqKicA18Ha0c4/ADOAz1flzX9eM9DGS5K6Ni5rnWXmmcCZI+xbCCys/r6acvFfkjRJuXqzJKlWBo0kqVYGjSSpVgaNJKlWBo0kqVYGjSSpVgaNJKlWBo0kqVYGjSSpVgaNJKlWBo0kqVYGjSSpVgaNJKlWBo0kqVYGjSSpVgaNJKlWBo0kqVYGjSSpVgaNJKlWBo0kqVYGjSSpVgaNJKlWBo0kqVYGjSSpVgaNJKlWBo0kqVYGjSSpVgaNJKlWBo0kqVbrjceHRsQhwNuBrYCbgKMy87p11N8JOAvYFbgHOAc4PTPXDKC5kqQxGPiIJiIOBM4DLgL2B5YDSyJi+xHqPwm4ElgD/DNwPvBe4OiBNFiSNCYDDZqIGAJOBc7PzFMyczGwL/B74MgRDnsLZeS1b2Yuzsz3AO8Hjo+ImYNotySpd4Me0ewAbAtc2ijIzNXAImDvEY7ZA7gqM1c2lV0CbArsUlM7JUl9MuhrNDtW21taypcBcyNiRmY+1OaYb7ap39h3bT8bODw83M+3m7JWrVoF2F/9Yn/2l/3ZP42+HItBj2g2rrYrWspXVG3ZcIRj2tVvfj9J0gQ16BHNULVtnS3WKH94hGNGml3Wrv6YzJs3r99vOSU1flO0v/rD/uwv+7N/hoeHWbly5egV12HQI5r7qu1GLeWzKKHxwAjHtNbfqGlf124/bUFX5ZKk3g06aJZW2zkt5XOAHOG+mKUj1AfIXhvSGiqGjCTVY9CnzpYCdwD7AVcAVFOUF1BmnrVzFXBoRGyYmY0Rz37A3ZSbPXtmuEhS/YbWrBnszfUR8WbgbMq9MNcAhwHPBf42M5dFxFxgdmZeX9XfAhgGfgScATwdOAU4LjM/2O3n33jjja4mIEldmj9//tDotdobeNAARMTRwFuBzSijkqMbS9BExKeBgzJzqKn+syhL0MwH7gTOzcwPDLrdkqTujUvQSJKmD1dvliTVyqCRJNXKoJEk1cqgkSTVyqCRJNXKoJEk1cqgkSTVyqCRJNXKoJEk1WrQi2qOm4g4BHg7sBVl2ZujGsveaGQRMYOyXNAhwDbA/wDnAudk5pqIGALeCRxKWVLoGuDwzPz5ODV5UoiIDSj/D7+XmQurMvuySxHxYuB9wNOAu4BPA6dm5kP2Z3eqn/WjgTcAmwM/A47PzKur/T3357QY0UTEgcB5wEXA/sByYElEbD+uDZscTqD8IF8E7At8CfgIcGy1/0TgXcAHgQOAxwNXRcTjB9/USeUk4CktZfZlFyJiN+AyyqK7CyiL9b6D0odgf3brWMrP+qcoK+TfClweEc+o9vfcn1N+rbMqhW8DLsvMN1VlMynPsvl6Zh4xnu2byCLiMZRQPiszT2gqPwd4JTAX+DXwnsYipxHxBMqo5+TM/NDgWz3xVT+43wFWAYsyc2FEbIR92ZWI+A5wX2bu01R2GvB3wMuwP7sSEcPADzLzwOr1DMp356XA8YyhP6fDiGYHYFtKZwGQmaspz7/Ze7waNUk8HrgQ+EpLeQKzgRdRno7a3Lf3At/Cvm0rItaj/MZ4BvCrpl1/h33ZsYiYDewGnN9cnpnHZeYLsT97sQFwf+NFZj5EeYrxpoyxP6fDNZodq+0tLeXLgLkRMaPqULWo/iMd1mbXy4BfUq53QRliN1sG/GONTZvM3gGsT3ke08ubyhv/T+3LzuwMDAEPRMTXgD0pX5LnAqdif/biHODEiPgqcAOwEPgb4P8yxv6cDiOajavtipbyFZR//4aDbc7kFhEHA3sAp1P69sHM/FNLtRU80u+qRMRTKD+0B7fpM/uyO7Or7YXAz4GXUkLmXZRrDfZn9z4GfBe4knLK/CPACZl5KWPsz+kwomk8QK31YlSj/OEBtmVSi4jXUCZVXEy58Ho8f9mvUPrWfm1SXe+6ALhghNmOQ9iX3ZhZbZdkZmNiyn9GxGaUsDkN+7Nj1bXsJcBTgTdTJljsAZwUEcsZ4//P6TCiua/abtRSPovSQQ8MtjmTU0QcCXwW+DrwmsxcQ+nbDarJFc1m8Ui/qziccq3wxIhYr7pWAzBU/d2+7M4fqu3lLeXfoPTZcuzPbuwGPBd4Y2Z+LDO/mZnvAj5EOXvxAGPoz+kQNEur7ZyW8jlAVl+YWoeIeB/lP9xngX9qGj4vpfxG0zpNfA5lwoAe8XLgycA9wOrqz9OBA5te25eda1xzXb+lvPFFaH92Z+tqe31L+XeBx1FGMz3353QJmjso88KBtdObFwBXjVejJouIeCvlFNlZwMLM/HPT7muBP/Lovn0C8ALs21aHAru0/LmZMkLcBfgi9mU3/psya++VLeULKNNw7c/u3Fxtd2sp3xX4M2Xmac/9OeXvowGIiDdTrim8n3I362GUYeLfZuay8WzbRBYRW1Dm0d9MuVu41Q2UG7zeRrlj+GbKxe4nA3+TmZ6iWIeIuAm4qWllgNOxLztW3Yj9GR65brgHZVbfmzLz4/ZndyLi68BzKNe4hoEXUn7J/GhmHjOW/pwOkwHIzHMj4q8oS6kcSVn6Yy9DZlR7UebW7wy0u4A9m/Kf7mHgGMr52muBg/xB7ol92YXMvDAiVlP67fWUMxdvzMzGvTX2Z3deCbyHEiCbUs4GHQF8vNrfc39OixGNJGn8TIdrNJKkcWTQSJJqZdBIkmpl0EiSamXQSJJqZdBI6lm1Rpa0TtPiPhpNHxHxOeBfgE0z8/6WfT8EngGcmJnvbtl3COXZJi9uPLq2hrZ9GjggMx/bQd3NgbdQlmDfjnJ39i2URxVfkJkP1tHGTkXEkymrRZwBfG8826KJzxGNpprLgRm0LKUREVtSQub3lOfptNqdslDjd+tu4GgiYnfgp5TngXyB8tjc1wPfpyzdfllEzBq3BhZ7Uh6L7ohGo3JEo6nmCsoCgM+nPE++YR/KqOAM4LSI2Dwzf9u0/4XAlW2etzFQEbEVZTmVO4AXZeY9Tbv/IyKuAy6iLBNy3Dg0UeqaQaMpJTPvjIgfURb7a7YPZbTyJeADlMUXLwCIiHnAFsDiRuWIeBxwNPAqyoq1dwOXUE673VPVWQj8G/Ba4JTqPd6WmZ+IiPmUtfX+HlhJWWuvk9/+D6cs//GPLSHT+Pd9LiKewSOrkjeuk7yKssTSPEqgfpPy0KqfVXW2o6xbd3xmntZ07AGUUdPumfnNpn/TrsBJlMB+mPKskrdl5q8j4uRqH8B1EfGt6vHJUlueOtNUdDnwrCosiIjHAi8GFmXm7ZQFA/dpqr97tb2sqr8+8J+UBQW/QFni/6OUJf2viYjWJwp+FDgROBhYUgXXd4DNq7IjgFdQToGN5mXA7zJzxFN4mXlMZl7QVPQh4HOU022vBo6irE/3vSqUenEp8CPK6bFTKdeKGp/5KcpiqlBWpT68x8/QNOGIRlPREspppb+nLGH+IsozNRZV+xcDb4yIDaqL6rsDP8nMX1b7DwSeTXnA2+erssurkdLllJFD82SCs5rqNSYkPAzskZl3VWVLKM9Xf9wobd+GEhgdiYgdq/Z8IjPf0FS+mLLC7gcpIdut8zPzxOrvV0TE04HXRsTjMvMXEdEYUf04M3/Sw/trGnFEo6noGsqzzJ9fvd4HuD0zh6vXi4ANgedVp51eSNNpM8py8w9SnmmyVmYuoVw72aPl837c8np34NuNkKmOXU559sxoHqK7XwBfRDkl9+mWtt5FGZU8vxqhdetbLa9vrz5nvCchaBIyaDTlZOZqyqmvxnWaBTwymoFyreZ+ym/6OwOb8eiJA08EfpuZ7Z6F/htgk5ay37a83gy4s82xv+6g+bcB263r/pSIeHJ1OrDR1pHe+zeU0Nqwg89t1fqI80Zf+J2hrvmfRlPVEmCXiHga5XTU2qCpguhKShDtTnnm+TVNx94NbB4R7X4+nkyZIr0ud1EmBrSa3UG7F1HC43nrqPMV4JfVM5bursq2bFPvyZSJAcspM/GgTP1u1nq9Seo7g0ZT1eWU6yFHAKsoI5xmi4CnUU49faPlEdVXUR749qiL9xHxEsqXd+t7tVoCvCAiGs9hpwqFvTto9zmU4DurelTuo0TEQZTrRxdm5irgakqILGypN5syseBbmbmGMoKDErrNdqc3D/V4nKYhJwNoSsrMZRFxC/A64IrM/GNLlcsoQfRS4I0t+y6kzKb6ZETMBX4A7AScQLk7/+xRPv4Uyky1qyPiFMr05qP4y1Nu7dr96+oRxV8E/isizqU8EXZjyrWm11JGY8dX9W+OiHOAwyJiBvBVyojoncD6VPfaZOa9EfFt4MCI+AmQVRt7DZp7q+3LI+JPmfnDHt9H04AjGk1lSyhftotbd2Tmb4D/Amby6OszNM1EOw84hHJR/QjK8+l3rS7sjygzf0GZ8TZMGaF8khIW53bS6My8FHhW1a6DKafKPg4EJRT/oWUJmiMoU4yfDXwZOBP4CbBLZt7QVO9Aykjv/ZSbQh9PmQ7di6ur9r2V0i/SiHyUsySpVo5oJEm1MmgkSbUyaCRJtTJoJEm1MmgkSbUyaCRJtTJoJEm1MmgkSbX6/6GeHnB4EoWRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a30d53358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate reduced plot\n",
    "plt.plot(x1, y1, marker='.', linestyle='none')\n",
    "\n",
    "# Make the margins nice\n",
    "plt.margins(.02)\n",
    "\n",
    "# Label the axes\n",
    "plt.xlabel('Word Count')\n",
    "plt.ylabel('ECDF')\n",
    "\n",
    "#Limit axes\n",
    "plt.xlim(0,80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 10)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a330317f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate reduced plot\n",
    "plt.plot(x1, y1, marker='.', linestyle='none')\n",
    "\n",
    "# Make the margins nice\n",
    "plt.margins(.02)\n",
    "\n",
    "# Label the axes\n",
    "plt.xlabel('Word Count')\n",
    "plt.ylabel('ECDF')\n",
    "\n",
    "#Limit axes\n",
    "plt.xlim(0,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph above indicates that 60% of the distinct words in the entire corpus only appear once. This is likely due to misspellings. The graph suggests that we try df_mins of 2, 3 and 4.\n",
    "\n",
    "There are two options for applying df_mins, percentages, and word counts. I will start with percentages. Anything between 0 and 1 is treated as a percentage. So, for instance, a df_min of 0.1 means that the classifier will discount all words that appear in less than 10% of reviews. A df_min of 2, however, means that the classifer will discount all words that appear in less than 2 reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try min_df by Percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_df():\n",
    "    min_dfs = [1e-4, 1e-3, 1e-2, 0.1, 1]\n",
    "    for val in min_dfs:\n",
    "        updated_vectorizers = [CountVectorizer(ngram_range=(1,2), min_df=val), CountVectorizer(ngram_range=(1,3), min_df=val), TfidfVectorizer(min_df=val, ngram_range=(1, 2)), TfidfVectorizer(min_df=val, ngram_range=(1, 3)) ]\n",
    "        for vect in updated_vectorizers:\n",
    "            print(str(vect))\n",
    "            for xy in xy_options:\n",
    "                print(str(xy))\n",
    "                run_updated_tests(vect, xy)\n",
    "                print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=0.0001,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "<function make_xy at 0x1ab669e0d0>\n",
      "Logistic Regression AUC: 0.9115601044214886\n",
      "Logistic Regression AUC cross-validation: [0.87538742 0.89748128 0.90553885 0.91295185 0.92559887]\n",
      "Logistic Regression AUC cross-valication mean: 0.9033916538034706\n",
      "Confusion Matrix: [[ 955  357]\n",
      " [ 304 3724]]\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.73      0.74      1312\n",
      "          1       0.91      0.92      0.92      4028\n",
      "\n",
      "avg / total       0.87      0.88      0.88      5340\n",
      "\n",
      "\n",
      "\n",
      "<function make_xy_norm at 0x1ab669e158>\n",
      "Logistic Regression AUC: 0.9056791641104118\n",
      "Logistic Regression AUC cross-validation: [0.8673985  0.88893134 0.89252468 0.90635291 0.92258405]\n",
      "Logistic Regression AUC cross-valication mean: 0.8955582968404041\n",
      "Confusion Matrix: [[ 933  361]\n",
      " [ 318 3728]]\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.72      0.73      1294\n",
      "          1       0.91      0.92      0.92      4046\n",
      "\n",
      "avg / total       0.87      0.87      0.87      5340\n",
      "\n",
      "\n",
      "\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=0.0001,\n",
      "        ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "<function make_xy at 0x1ab669e0d0>\n",
      "Logistic Regression AUC: 0.914939205875625\n",
      "Logistic Regression AUC cross-validation: [0.8778488  0.90020129 0.90813787 0.91396143 0.92698254]\n",
      "Logistic Regression AUC cross-valication mean: 0.9054263851350994\n",
      "Confusion Matrix: [[ 977  332]\n",
      " [ 310 3721]]\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.75      0.75      1309\n",
      "          1       0.92      0.92      0.92      4031\n",
      "\n",
      "avg / total       0.88      0.88      0.88      5340\n",
      "\n",
      "\n",
      "\n",
      "<function make_xy_norm at 0x1ab669e158>\n",
      "Logistic Regression AUC: 0.9125243152816682\n",
      "Logistic Regression AUC cross-validation: [0.86811223 0.88980743 0.89339193 0.90685875 0.92297446]\n",
      "Logistic Regression AUC cross-valication mean: 0.8962289612071398\n",
      "Confusion Matrix: [[ 931  329]\n",
      " [ 317 3763]]\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.74      0.74      1260\n",
      "          1       0.92      0.92      0.92      4080\n",
      "\n",
      "avg / total       0.88      0.88      0.88      5340\n",
      "\n",
      "\n",
      "\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=0.0001,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "<function make_xy at 0x1ab669e0d0>\n",
      "Logistic Regression AUC: 0.905520493985904\n",
      "Logistic Regression AUC cross-validation: [0.86904324 0.89240733 0.9012165  0.90910695 0.92452523]\n",
      "Logistic Regression AUC cross-valication mean: 0.8992598525229394\n",
      "Confusion Matrix: [[ 732  553]\n",
      " [ 110 3945]]\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.57      0.69      1285\n",
      "          1       0.88      0.97      0.92      4055\n",
      "\n",
      "avg / total       0.88      0.88      0.87      5340\n",
      "\n",
      "\n",
      "\n",
      "<function make_xy_norm at 0x1ab669e158>\n",
      "Logistic Regression AUC: 0.9126610993023213\n",
      "Logistic Regression AUC cross-validation: [0.85899734 0.89478451 0.901716   0.91345105 0.92524321]\n",
      "Logistic Regression AUC cross-valication mean: 0.8988384219308777\n",
      "Confusion Matrix: [[ 596  653]\n",
      " [  70 4021]]\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.48      0.62      1249\n",
      "          1       0.86      0.98      0.92      4091\n",
      "\n",
      "avg / total       0.87      0.86      0.85      5340\n",
      "\n",
      "\n",
      "\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=0.0001,\n",
      "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "<function make_xy at 0x1ab669e0d0>\n",
      "Logistic Regression AUC: 0.9053296387157146\n",
      "Logistic Regression AUC cross-validation: [0.87012938 0.88889301 0.89812463 0.90587819 0.92300891]\n",
      "Logistic Regression AUC cross-valication mean: 0.8972068236628555\n",
      "Confusion Matrix: [[ 685  547]\n",
      " [ 103 4005]]\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.56      0.68      1232\n",
      "          1       0.88      0.97      0.92      4108\n",
      "\n",
      "avg / total       0.88      0.88      0.87      5340\n",
      "\n",
      "\n",
      "\n",
      "<function make_xy_norm at 0x1ab669e158>\n",
      "Logistic Regression AUC: 0.9093313364447763\n",
      "Logistic Regression AUC cross-validation: [0.86031645 0.89457296 0.9018541  0.91369098 0.92558165]\n",
      "Logistic Regression AUC cross-valication mean: 0.8992032263107133\n",
      "Confusion Matrix: [[ 551  735]\n",
      " [  67 3987]]\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.43      0.58      1286\n",
      "          1       0.84      0.98      0.91      4054\n",
      "\n",
      "avg / total       0.86      0.85      0.83      5340\n",
      "\n",
      "\n",
      "\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=0.001,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "<function make_xy at 0x1ab669e0d0>\n",
      "Logistic Regression AUC: 0.9104639690412437\n",
      "Logistic Regression AUC cross-validation: [0.87044173 0.89316723 0.8999389  0.90824303 0.91979828]\n",
      "Logistic Regression AUC cross-valication mean: 0.8983178324078029\n",
      "Confusion Matrix: [[ 944  325]\n",
      " [ 349 3722]]\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      0.74      0.74      1269\n",
      "          1       0.92      0.91      0.92      4071\n",
      "\n",
      "avg / total       0.87      0.87      0.87      5340\n",
      "\n",
      "\n",
      "\n",
      "<function make_xy_norm at 0x1ab669e158>\n",
      "Logistic Regression AUC: 0.9044350095519922\n",
      "Logistic Regression AUC cross-validation: [0.86169804 0.87895847 0.88076818 0.89539476 0.91160747]\n",
      "Logistic Regression AUC cross-valication mean: 0.885685383840156\n",
      "Confusion Matrix: [[ 922  317]\n",
      " [ 391 3710]]\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.70      0.74      0.72      1239\n",
      "          1       0.92      0.90      0.91      4101\n",
      "\n",
      "avg / total       0.87      0.87      0.87      5340\n",
      "\n",
      "\n",
      "\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=0.001,\n",
      "        ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "<function make_xy at 0x1ab669e0d0>\n",
      "Logistic Regression AUC: 0.9152337338924076\n",
      "Logistic Regression AUC cross-validation: [0.8721432  0.89501206 0.90197618 0.90836269 0.92079758]\n",
      "Logistic Regression AUC cross-valication mean: 0.8996583412489632\n",
      "Confusion Matrix: [[ 922  336]\n",
      " [ 334 3748]]\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      0.73      0.73      1258\n",
      "          1       0.92      0.92      0.92      4082\n",
      "\n",
      "avg / total       0.87      0.87      0.87      5340\n",
      "\n",
      "\n",
      "\n",
      "<function make_xy_norm at 0x1ab669e158>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression AUC: 0.8990643229312524\n",
      "Logistic Regression AUC cross-validation: [0.86156646 0.87927444 0.88094979 0.89530834 0.91158057]\n",
      "Logistic Regression AUC cross-valication mean: 0.8857359206744352\n",
      "Confusion Matrix: [[ 908  378]\n",
      " [ 374 3680]]\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.71      0.71      0.71      1286\n",
      "          1       0.91      0.91      0.91      4054\n",
      "\n",
      "avg / total       0.86      0.86      0.86      5340\n",
      "\n",
      "\n",
      "\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=0.001,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "<function make_xy at 0x1ab669e0d0>\n",
      "Logistic Regression AUC: 0.9189444745642359\n",
      "Logistic Regression AUC cross-validation: [0.8788622  0.89914232 0.90735976 0.91676744 0.9288923 ]\n",
      "Logistic Regression AUC cross-valication mean: 0.9062048050194779\n",
      "Confusion Matrix: [[ 784  501]\n",
      " [ 141 3914]]\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.61      0.71      1285\n",
      "          1       0.89      0.97      0.92      4055\n",
      "\n",
      "avg / total       0.88      0.88      0.87      5340\n",
      "\n",
      "\n",
      "\n",
      "<function make_xy_norm at 0x1ab669e158>\n",
      "Logistic Regression AUC: 0.9176720385381802\n",
      "Logistic Regression AUC cross-validation: [0.86539252 0.89730655 0.90526205 0.91507163 0.92481804]\n",
      "Logistic Regression AUC cross-valication mean: 0.9015701573307616\n",
      "Confusion Matrix: [[ 688  582]\n",
      " [ 129 3941]]\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.54      0.66      1270\n",
      "          1       0.87      0.97      0.92      4070\n",
      "\n",
      "avg / total       0.86      0.87      0.86      5340\n",
      "\n",
      "\n",
      "\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=0.001,\n",
      "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "<function make_xy at 0x1ab669e0d0>\n",
      "Logistic Regression AUC: 0.9164779701022816\n",
      "Logistic Regression AUC cross-validation: [0.88033885 0.89912663 0.90701105 0.91592679 0.92841365]\n",
      "Logistic Regression AUC cross-valication mean: 0.9061633921850399\n",
      "Confusion Matrix: [[ 756  484]\n",
      " [ 146 3954]]\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.61      0.71      1240\n",
      "          1       0.89      0.96      0.93      4100\n",
      "\n",
      "avg / total       0.88      0.88      0.88      5340\n",
      "\n",
      "\n",
      "\n",
      "<function make_xy_norm at 0x1ab669e158>\n",
      "Logistic Regression AUC: 0.9157805527232279\n",
      "Logistic Regression AUC cross-validation: [0.86559894 0.89760471 0.90550047 0.91519401 0.92508003]\n",
      "Logistic Regression AUC cross-valication mean: 0.9017956324526548\n",
      "Confusion Matrix: [[ 705  553]\n",
      " [ 113 3969]]\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.56      0.68      1258\n",
      "          1       0.88      0.97      0.92      4082\n",
      "\n",
      "avg / total       0.87      0.88      0.87      5340\n",
      "\n",
      "\n",
      "\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=0.01,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "<function make_xy at 0x1ab669e0d0>\n",
      "Logistic Regression AUC: 0.8728253232044028\n",
      "Logistic Regression AUC cross-validation: [0.83814081 0.8662665  0.87450586 0.8832134  0.88849636]\n",
      "Logistic Regression AUC cross-valication mean: 0.8701245859113766\n",
      "Confusion Matrix: [[ 875  393]\n",
      " [ 479 3593]]\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.65      0.69      0.67      1268\n",
      "          1       0.90      0.88      0.89      4072\n",
      "\n",
      "avg / total       0.84      0.84      0.84      5340\n",
      "\n",
      "\n",
      "\n",
      "<function make_xy_norm at 0x1ab669e158>\n",
      "Logistic Regression AUC: 0.8774439185428364\n",
      "Logistic Regression AUC cross-validation: [0.83195356 0.86548034 0.87502017 0.88707009 0.89132776]\n",
      "Logistic Regression AUC cross-valication mean: 0.8701703832859184\n",
      "Confusion Matrix: [[ 820  394]\n",
      " [ 437 3689]]\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.65      0.68      0.66      1214\n",
      "          1       0.90      0.89      0.90      4126\n",
      "\n",
      "avg / total       0.85      0.84      0.85      5340\n",
      "\n",
      "\n",
      "\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=0.01,\n",
      "        ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "<function make_xy at 0x1ab669e0d0>\n",
      "Logistic Regression AUC: 0.8777565167655736\n",
      "Logistic Regression AUC cross-validation: [0.83775875 0.86356339 0.87265292 0.8801741  0.87939811]\n",
      "Logistic Regression AUC cross-valication mean: 0.8667094520294782\n",
      "Confusion Matrix: [[ 875  396]\n",
      " [ 420 3649]]\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.68      0.69      0.68      1271\n",
      "          1       0.90      0.90      0.90      4069\n",
      "\n",
      "avg / total       0.85      0.85      0.85      5340\n",
      "\n",
      "\n",
      "\n",
      "<function make_xy_norm at 0x1ab669e158>\n",
      "Logistic Regression AUC: 0.8762212199733231\n",
      "Logistic Regression AUC cross-validation: [0.83217055 0.86554402 0.87531056 0.88708519 0.89140693]\n",
      "Logistic Regression AUC cross-valication mean: 0.8703034500392729\n",
      "Confusion Matrix: [[ 878  414]\n",
      " [ 430 3618]]\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.67      0.68      0.68      1292\n",
      "          1       0.90      0.89      0.90      4048\n",
      "\n",
      "avg / total       0.84      0.84      0.84      5340\n",
      "\n",
      "\n",
      "\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=0.01,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "<function make_xy at 0x1ab669e0d0>\n",
      "Logistic Regression AUC: 0.9143598911782285\n",
      "Logistic Regression AUC cross-validation: [0.87743505 0.90164535 0.91003403 0.91835629 0.92621138]\n",
      "Logistic Regression AUC cross-valication mean: 0.9067364193202578\n",
      "Confusion Matrix: [[ 789  442]\n",
      " [ 192 3917]]\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.64      0.71      1231\n",
      "          1       0.90      0.95      0.93      4109\n",
      "\n",
      "avg / total       0.88      0.88      0.88      5340\n",
      "\n",
      "\n",
      "\n",
      "<function make_xy_norm at 0x1ab669e158>\n",
      "Logistic Regression AUC: 0.9059497498292917\n",
      "Logistic Regression AUC cross-validation: [0.85983933 0.89147903 0.90104094 0.91271676 0.91970068]\n",
      "Logistic Regression AUC cross-valication mean: 0.896955346568199\n",
      "Confusion Matrix: [[ 800  516]\n",
      " [ 206 3818]]\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.61      0.69      1316\n",
      "          1       0.88      0.95      0.91      4024\n",
      "\n",
      "avg / total       0.86      0.86      0.86      5340\n",
      "\n",
      "\n",
      "\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=0.01,\n",
      "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "<function make_xy at 0x1ab669e0d0>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression AUC: 0.9099085232918621\n",
      "Logistic Regression AUC cross-validation: [0.87786147 0.90197882 0.91045527 0.91810669 0.92641293]\n",
      "Logistic Regression AUC cross-valication mean: 0.9069630372338814\n",
      "Confusion Matrix: [[ 792  479]\n",
      " [ 190 3879]]\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      0.62      0.70      1271\n",
      "          1       0.89      0.95      0.92      4069\n",
      "\n",
      "avg / total       0.87      0.87      0.87      5340\n",
      "\n",
      "\n",
      "\n",
      "<function make_xy_norm at 0x1ab669e158>\n",
      "Logistic Regression AUC: 0.915488262248062\n",
      "Logistic Regression AUC cross-validation: [0.85994918 0.89154331 0.90115698 0.91275967 0.91984542]\n",
      "Logistic Regression AUC cross-valication mean: 0.897050910434273\n",
      "Confusion Matrix: [[ 803  496]\n",
      " [ 165 3876]]\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.62      0.71      1299\n",
      "          1       0.89      0.96      0.92      4041\n",
      "\n",
      "avg / total       0.87      0.88      0.87      5340\n",
      "\n",
      "\n",
      "\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "<function make_xy at 0x1ab669e0d0>\n",
      "Logistic Regression AUC: 0.918288732438792\n",
      "Logistic Regression AUC cross-validation: [0.87729592 0.89903851 0.90653331 0.91447664 0.92714541]\n",
      "Logistic Regression AUC cross-valication mean: 0.9048979577858685\n",
      "Confusion Matrix: [[ 934  302]\n",
      " [ 267 3837]]\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      0.76      0.77      1236\n",
      "          1       0.93      0.93      0.93      4104\n",
      "\n",
      "avg / total       0.89      0.89      0.89      5340\n",
      "\n",
      "\n",
      "\n",
      "<function make_xy_norm at 0x1ab669e158>\n",
      "Logistic Regression AUC: 0.9081074816625916\n",
      "Logistic Regression AUC cross-validation: [0.86931002 0.89291705 0.89733021 0.91076651 0.92543569]\n",
      "Logistic Regression AUC cross-valication mean: 0.8991518957049724\n",
      "Confusion Matrix: [[ 909  341]\n",
      " [ 301 3789]]\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.73      0.74      1250\n",
      "          1       0.92      0.93      0.92      4090\n",
      "\n",
      "avg / total       0.88      0.88      0.88      5340\n",
      "\n",
      "\n",
      "\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "<function make_xy at 0x1ab669e0d0>\n",
      "Logistic Regression AUC: 0.919685988826764\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-192-0a467799be3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmin_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-191-4d9bac08cf53>\u001b[0m in \u001b[0;36mmin_df\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mxy\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxy_options\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                 \u001b[0mrun_updated_tests\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-184-a3b95f6f07bc>\u001b[0m in \u001b[0;36mrun_updated_tests\u001b[0;34m(vectorizer, xy_func)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_updated_tests\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxy_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxy_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_adjusted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mupdated_tester\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-182-f66515faf7a1>\u001b[0m in \u001b[0;36mupdated_tester\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Compute cross-validated AUC scores: cv_auc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mcv_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogreg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'roc_auc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Print list of AUC scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[1;32m    340\u001b[0m                                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                                 pre_dispatch=pre_dispatch)\n\u001b[0m\u001b[1;32m    343\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score)\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_train_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             return_times=True)\n\u001b[0;32m--> 206\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1231\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1233\u001b[0;31m                 sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m   1234\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_iter_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[0;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mclass_weight_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m         epsilon, sample_weight)\n\u001b[0m\u001b[1;32m    891\u001b[0m     \u001b[0;31m# Regarding rnd.randint(..) in the above signature:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m     \u001b[0;31m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "min_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation Means over 0.9\n",
    "\n",
    "0.9033916538034706: CV(1,2, df_min 0.0001, make_xy)\n",
    "\n",
    "0.9054263851350994: CV(1,3, df_min 0.0001, make_xy)*****0.75 0s correct\n",
    "\n",
    "0.9062048050194779: Tfdif(1,2, df_mi 0.001, make_xy)*\n",
    "\n",
    "0.9015701573307616: Tfdif(1,2, df_mi 0.001, make_xy_norm)\n",
    "\n",
    "0.9061633921850399: Tfdif(1,3, df_mi 0.001, make_xy)\n",
    "\n",
    "0.9017956324526548: Tfdif(1,3, df_mi 0.001, make_xy_norm)\n",
    "\n",
    "0.9067364193202578: Tfdif(1,2, df_mi 0.01, make_xy)**\n",
    "\n",
    "0.9069630372338814: Tfdif(1,3, df_mi 0.01, make_xy)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Min_df by Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_df_r():\n",
    "    min_dfs = [1, 2, 3, 4]\n",
    "    for val in min_dfs:\n",
    "        updated_vectorizers = [CountVectorizer(ngram_range=(1,2), min_df=val), CountVectorizer(ngram_range=(1,3), min_df=val), TfidfVectorizer(min_df=val, ngram_range=(1, 2)), TfidfVectorizer(min_df=val, ngram_range=(1, 3)) ]\n",
    "        for vect in updated_vectorizers:\n",
    "            print(str(vect))\n",
    "            for xy in xy_options:\n",
    "                print(str(xy))\n",
    "                run_updated_tests(vect, xy)\n",
    "                print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "<function make_xy at 0x1ab669e0d0>\n",
      "Logistic Regression AUC: 0.9194473452721996\n",
      "AUC scores computed using 5-fold cross-validation: [0.88066828 0.90964066 0.91206725 0.91979915 0.92708747]\n",
      "Logistic Regression AUC cross-validation mean: 0.9098525640194273\n",
      "Confusion Matrix: [[1157  327]\n",
      " [ 313 3125]]\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.78      0.78      1484\n",
      "          1       0.91      0.91      0.91      3438\n",
      "\n",
      "avg / total       0.87      0.87      0.87      4922\n",
      "\n",
      "\n",
      "\n",
      "<function make_xy_norm at 0x1ab669e158>\n",
      "Logistic Regression AUC: 0.9150918769762062\n",
      "AUC scores computed using 5-fold cross-validation: [0.86943306 0.90356245 0.90363785 0.91447959 0.92681179]\n",
      "Logistic Regression AUC cross-validation mean: 0.903584947604263\n",
      "Confusion Matrix: [[1144  319]\n",
      " [ 331 3128]]\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      0.78      0.78      1463\n",
      "          1       0.91      0.90      0.91      3459\n",
      "\n",
      "avg / total       0.87      0.87      0.87      4922\n",
      "\n",
      "\n",
      "\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "<function make_xy at 0x1ab669e0d0>\n",
      "Logistic Regression AUC: 0.9170254753680389\n",
      "AUC scores computed using 5-fold cross-validation: [0.88223747 0.91082069 0.91450407 0.91951666 0.92776473]\n",
      "Logistic Regression AUC cross-validation mean: 0.9109687266287713\n",
      "Confusion Matrix: [[1158  287]\n",
      " [ 345 3132]]\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.80      0.79      1445\n",
      "          1       0.92      0.90      0.91      3477\n",
      "\n",
      "avg / total       0.87      0.87      0.87      4922\n",
      "\n",
      "\n",
      "\n",
      "<function make_xy_norm at 0x1ab669e158>\n",
      "Logistic Regression AUC: 0.9162674877061245\n"
     ]
    }
   ],
   "source": [
    "min_df_r()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have not yet tried random forests, a nice extension of decision trees. Here goes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forests Training score: 0.7592592592592593\n",
      "Random Forests Test CLF score: 0.7644918748484113\n",
      "[0.76053349 0.76046089 0.76046089 0.76069154 0.76069154]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Split into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=2,n_estimators=10)\n",
    "\n",
    "# Fit the classifier to the data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Random Forests Training score:\", clf.score(X_train, y_train))\n",
    "print(\"Random Forests Test CLF score:\", clf.score(X_test, y_test))\n",
    "    \n",
    "# Compute 5-fold cross-validation scores: cv_scores\n",
    "cv_scores = cross_val_score(clf, X, y, cv=5)\n",
    "\n",
    "print(cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forests performs almost identically to Decision Trees for this dataset. I will discard it moving forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check 'overall'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same tests that I have been running can also be used to predict the 'overall' column, which gives the star ratings. I presume that the predictions will be better for 'overall' since the bag of words approach is more likely to be effective with sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Stars Column (binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function\n",
    "def stars(row):\n",
    "    # Give 1 star reviews a value of 0\n",
    "    if row['overall']==1:\n",
    "        return 0\n",
    "    # Give 2 star reviews a value of 0\n",
    "    elif row['overall']==2:\n",
    "        return 0\n",
    "    # Give 3 star reviews a value of 0\n",
    "    elif row['overall']==3:\n",
    "        return 0\n",
    "    # Give 4,5 star reviews a value of 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# Create column\n",
    "df['Stars'] = df.apply(stars, axis=1)\n",
    "\n",
    "# Define new dataframe that eliminates the middle: 3-star reviews\n",
    "df_adjusted_overall = df[df['overall']!=3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>helpful</th>\n",
       "      <th>Review_Length</th>\n",
       "      <th>Sentence_Length</th>\n",
       "      <th>Word_Length</th>\n",
       "      <th>Helpful_Rating</th>\n",
       "      <th>Helpful</th>\n",
       "      <th>Stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>This is one of the first (literary) books I re...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[81, 92]</td>\n",
       "      <td>1542</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>4.258865</td>\n",
       "      <td>0.893237</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>The Prophet is Kahlil Gibran's best known work...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[8, 10]</td>\n",
       "      <td>2294</td>\n",
       "      <td>22.705882</td>\n",
       "      <td>4.883289</td>\n",
       "      <td>0.713138</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Gibran Khalil Gibran was born in 1883 in what ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[8, 10]</td>\n",
       "      <td>712</td>\n",
       "      <td>19.428571</td>\n",
       "      <td>4.103704</td>\n",
       "      <td>0.713138</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Certainly the words are of Kahlil Gibran, but ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[10, 12]</td>\n",
       "      <td>700</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>4.675000</td>\n",
       "      <td>0.754717</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>I evidently misread the writeup, I thought it ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[0, 13]</td>\n",
       "      <td>178</td>\n",
       "      <td>11.666667</td>\n",
       "      <td>3.885714</td>\n",
       "      <td>0.021653</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           reviewText  overall   helpful  \\\n",
       "14  This is one of the first (literary) books I re...      5.0  [81, 92]   \n",
       "18  The Prophet is Kahlil Gibran's best known work...      5.0   [8, 10]   \n",
       "19  Gibran Khalil Gibran was born in 1883 in what ...      5.0   [8, 10]   \n",
       "35  Certainly the words are of Kahlil Gibran, but ...      5.0  [10, 12]   \n",
       "36  I evidently misread the writeup, I thought it ...      2.0   [0, 13]   \n",
       "\n",
       "    Review_Length  Sentence_Length  Word_Length  Helpful_Rating  Helpful  \\\n",
       "14           1542        15.000000     4.258865        0.893237        1   \n",
       "18           2294        22.705882     4.883289        0.713138        0   \n",
       "19            712        19.428571     4.103704        0.713138        0   \n",
       "35            700        21.000000     4.675000        0.754717        1   \n",
       "36            178        11.666667     3.885714        0.021653        0   \n",
       "\n",
       "    Stars  \n",
       "14      1  \n",
       "18      1  \n",
       "19      1  \n",
       "35      1  \n",
       "36      0  "
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show new dataframe\n",
    "df_adjusted_overall.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjust Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_xy_overall(df, vectorizer):\n",
    "    vectorizer = vectorizer\n",
    "    X = vectorizer.fit_transform(df.reviewText)\n",
    "    X = X.tocsc()  # some versions of sklearn return COO format\n",
    "    y = df.Stars\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_updated_tests_overall(vectorizer, xy_func):\n",
    "    X,y = xy_func(df_adjusted_overall, vectorizer)\n",
    "    tester(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not a full sample, but enough to determine if it's worth going forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=0,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "Logistic Regression AUC: 0.9475864819208739\n",
      "Logistic Regression AUC cross-validation: [0.94328566 0.92990798 0.93660454 0.94683554 0.95010497]\n",
      "Logistic Regression AUC cross-valication mean: 0.9413477391050142\n",
      "Confusion Matrix: [[3483  659]\n",
      " [ 573 6721]]\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.84      0.85      4142\n",
      "          1       0.91      0.92      0.92      7294\n",
      "\n",
      "avg / total       0.89      0.89      0.89     11436\n",
      "\n",
      "\n",
      "\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=0,\n",
      "        ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-223-08b9b8be3d8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvect\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mupdated_vectorizers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mrun_updated_tests_overall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmake_xy_overall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-222-0a8cbebecc3c>\u001b[0m in \u001b[0;36mrun_updated_tests_overall\u001b[0;34m(vectorizer, xy_func)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_updated_tests_overall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxy_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxy_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_adjusted_overall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mupdated_tester\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-220-6730248fd425>\u001b[0m in \u001b[0;36mmake_xy_overall\u001b[0;34m(df, vectorizer)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_xy_overall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreviewText\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtocsc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# some versions of sklearn return COO format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 869\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    795\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfeature_idx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeature_counter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m                         \u001b[0mfeature_counter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "val = 0\n",
    "updated_vectorizers = [CountVectorizer(ngram_range=(1,2), min_df=val), CountVectorizer(ngram_range=(1,3), min_df=val)]\n",
    "for vect in updated_vectorizers:\n",
    "    print(str(vect))\n",
    "    run_updated_tests_overall(vect, make_xy_overall)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adjusted_overall_2 = df_adjusted_overall[df_adjusted_overall['overall']!=4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Multinomial Training CLF score: 0.8797877922229348\n",
      "Naive Bayes Multinomial Test CLF score: 0.8166316894018888\n",
      "Naive Bayes Multinomial Cross-validation scores: [0.80751995 0.79975954 0.79888512 0.79755138 0.78156773]\n",
      "Naive Bayes Multinomial Mean cross-validation scores: 0.7970567425255568\n",
      "Logistic Regression AUC cross-validation: [0.91987592 0.90047107 0.90485739 0.91944597 0.93058202]\n",
      "Logistic Regression AUC cross-validation mean: 0.9150464716582352\n",
      "Tuned Decision Tree Parameters: {'criterion': 'entropy', 'max_depth': 3, 'max_features': 8, 'min_samples_leaf': 7}\n",
      "Best Tuned Decision Tree score: 0.6370731494031743\n",
      "Random Forests Training score: 0.994257564274471\n",
      "Random Forests Test CLF score: 0.7360090940888423\n",
      "Random Forest cv_scores: [0.7341786  0.73024374 0.73242977 0.74365982 0.73663496]\n",
      "Random Forest cv_mean: 0.7354293787242523\n"
     ]
    }
   ],
   "source": [
    "run_updated_tests_overall(CountVectorizer(), make_xy_overall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression can be tuned by a parameter C. The lower the value of C, the more regularization. The default value is 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_regression(X, y):\n",
    "    \n",
    "    # Split into training and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    \n",
    "    # LOGISTIC REGRESSION TUNED\n",
    "    \n",
    "    # Setup the hyperparameter grid\n",
    "    c_space = np.logspace(-5, 8, 15)\n",
    "    param_grid = {'C': c_space}\n",
    "\n",
    "    # Instantiate a logistic regression classifier: logreg\n",
    "    logreg = LogisticRegression()\n",
    "\n",
    "    # Instantiate the GridSearchCV object: logreg_cv\n",
    "    logreg_cv = GridSearchCV(logreg, param_grid, cv=5)\n",
    "\n",
    "    # Fit it to the data\n",
    "    logreg_cv.fit(X,y)\n",
    "\n",
    "    # Print the tuned parameters and score\n",
    "    print(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_)) \n",
    "    print(\"Best Tuned Logistic Regression score: {}\".format(logreg_cv.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tune_regression(vect, xy_func):\n",
    "    X,y = xy_func(df_adjusted, vect)\n",
    "    tune_regression(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-236-38e23f699543>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrun_tune_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmake_xy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-235-c8061c267c8a>\u001b[0m in \u001b[0;36mrun_tune_regression\u001b[0;34m(vect, xy_func)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_tune_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxy_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxy_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_adjusted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtune_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-207-d3f05d3d58f9>\u001b[0m in \u001b[0;36mmake_xy\u001b[0;34m(df, vectorizer)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtocsc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# some versions of sklearn return COO format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHelpful\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 869\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 266\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,2))\n",
    "run_tune_regression(vect, make_xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check scikit learn linear regression example countvectorizer\n",
    "# check word2vec\n",
    "# boosted trees, random forests, \n",
    "\n",
    "# text blob (python package)- great sentiment analysis function\n",
    "# consider mean +- standard deviation, quartiles, etc.\n",
    "# distribution should determine it, harmonic mean\n",
    "\n",
    "# use probabilities after binary split to return 1-5, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# google countvectorizer other features, side features\n",
    "# google nlp feature engineering\n",
    "# pos tagging (before stripping punctuation)\n",
    "# sentence tokenizer\n",
    "# nltk\n",
    "# gensim\n",
    "# pyLBAvis (visualization of LBA models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
